{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11662423,"sourceType":"datasetVersion","datasetId":2027239}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Dataset Loading","metadata":{}},{"cell_type":"markdown","source":"## Definitions","metadata":{}},{"cell_type":"code","source":"# System Imports\nimport os, sys, shutil\nfrom os.path import join, realpath\nimport json\nimport xml.etree.ElementTree as ET\nfrom xml import etree\nfrom random import choice\nimport random\nfrom copy import deepcopy\nimport cv2\nfrom random import choice\n#import google.generativeai as genai\nfrom pathlib import Path\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\nfrom scipy.ndimage import zoom\nimport colorsys\nimport random\nimport ast\nimport tempfile\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nimport re\nfrom typing import Union, Dict\n\n\ndef show_image(example):\n    try:\n        image = read_image(example)\n    except:\n        image = cv2.imread(example)\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    # Display the image using Matplotlib\n    plt.figure(figsize=(10, 10))\n    plt.imshow(image_rgb)\n    plt.axis(\"off\")  # Hide the axis\n    plt.show()\n    \n\ndef load_classes() -> dict:\n    \"\"\"Returns the List of Classes as Encoding Map\"\"\"\n\n    with open(\"/kaggle/input/cghd1152/classes.json\") as classes_file:\n        return json.loads(classes_file.read())\n\n\ndef load_classes_ports() -> dict:\n    \"\"\"Reads Symbol Library from File\"\"\"\n\n    with open(\"/kaggle/input/cghd1152/classes_ports.json\") as json_file:\n        return json.loads(json_file.read())\n\n\ndef load_properties() -> dict:\n    \"\"\"Loads the Properties RegEx File\"\"\"\n\n    with open(\"/kaggle/input/cghd1152/properties.json\") as json_file:\n        return json.loads(json_file.read())\n\n\ndef _sample_info_from_path(path: str) -> tuple:\n    \"\"\"Extracts Sample Metadata from File Path\"\"\"\n\n    drafter, _, file_name = os.path.normpath(path).split(os.sep)[-3:]\n    circuit, drawing, picture = file_name.split(\"_\")\n    picture, suffix = picture.split(\".\")\n    return drafter.split(\"_\")[1], int(circuit[1:]), int(drawing[1:]), int(picture[1:]), suffix\n\n\ndef sample_name(sample: dict) -> str:\n    \"\"\"Returns the Name of a Sample\"\"\"\n\n    return f\"C{sample['circuit']}_D{sample['drawing']}_P{sample['picture']}\"\n\n\ndef file_name(sample: dict) -> str:\n    \"\"\"return the Raw Image File Name of a Sample\"\"\"\n\n    return f\"{sample_name(sample)}.{sample['format']}\"\n\n\ndef read_pascal_voc(path: str) -> dict:\n    \"\"\"Reads the Content of a Pascal VOC Annotation File\"\"\"\n\n    root = ET.parse(path).getroot()\n    circuit, drawing, picture = root.find(\"filename\").text.split(\"_\")\n    drafter = int(os.path.normpath(path).split(os.sep)[-3].split(\"_\")[1])\n\n    return {\"drafter\": drafter,\n            \"circuit\": int(circuit[1:]),\n            \"drawing\": int(drawing[1:]),\n            \"picture\": int(picture.split(\".\")[0][1:]),\n            \"format\": picture.split(\".\")[-1],\n            \"width\": int(root.find(\"size/width\").text),\n            \"height\": int(int(root.find(\"size/height\").text)),\n            \"bboxes\": [{\"class\": annotation.find(\"name\").text,\n                        \"xmin\": int(annotation.find(\"bndbox/xmin\").text),\n                        \"xmax\": int(annotation.find(\"bndbox/xmax\").text),\n                        \"ymin\": int(annotation.find(\"bndbox/ymin\").text),\n                        \"ymax\": int(annotation.find(\"bndbox/ymax\").text),\n                        \"rotation\": int(annotation.find(\"bndbox/rotation\").text) if annotation.find(\"bndbox/rotation\") is not None else None,\n                        \"text\": annotation.find(\"text\").text if annotation.find(\"text\") is not None else None}\n                       for annotation in root.findall('object')],\n            \"polygons\": [], \"points\": []}\n\n\ndef read_labelme(path: str) -> dict:\n    \"\"\"Reads and Returns Geometric Objects from a LabelME JSON File\"\"\"\n\n    with open(path) as json_file:\n        json_data = json.load(json_file)\n\n    drafter, circuit, drawing, picture, _ = _sample_info_from_path(path)\n    suffix = json_data['imagePath'].split(\".\")[-1]\n\n    return {'img_path': json_data['imagePath'].replace(\"\\\\\", \"/\"), 'drafter': drafter, 'circuit': circuit,\n            'drawing': drawing, 'picture': picture, 'format': suffix,\n            'height': json_data['imageHeight'], 'width': json_data['imageWidth'], 'bboxes': [],\n            'polygons': [{'class': shape['label'],\n                          'bbox': {'xmin': min(point[0] for point in shape['points']),\n                                   'ymin': min(point[1] for point in shape['points']),\n                                   'xmax': max(point[0] for point in shape['points']),\n                                   'ymax': max(point[1] for point in shape['points'])},\n                          'points': shape['points'],\n                          'rotation': shape.get('rotation', None),\n                          'text': shape.get('text', None),\n                          'group': shape.get('group_id', None)}\n                         for shape in json_data['shapes']\n                         if shape['shape_type'] == \"polygon\"],\n            'points': [{'class': shape['label'], 'points': shape['points'][0],\n                        'group': shape['group_id'] if 'group_id' in shape else None}\n                       for shape in json_data['shapes']\n                       if shape['shape_type'] == \"point\"]}\n\n\ndef read_dataset(drafter: int = None, circuit: int = None, segmentation=False, folder: str = None) -> list:\n    \"\"\"Reads all BB Annotation Files from Folder Structure\n    This Method can be invoked from Anywhere, can be restricted to a specified drafter\n    and can be use for both BB and Polygon Annotations. Alternative annotation sub-folder\n    can be specified to read processed ground truth.\"\"\"\n\n    db_root = os.sep.join(realpath('/kaggle/input/cghd1152').split(os.sep))\n\n    return sorted([(read_labelme if segmentation else read_pascal_voc)(join(root, file_name))\n                   for root, _, files in os.walk(db_root)\n                   for file_name in files\n                   if (folder if folder else (\"instances\" if segmentation else \"annotations\")) in root and\n                      (not circuit or f\"C{circuit}_\" in file_name) and\n                      (drafter is None or f\"drafter_{drafter}{os.sep}\" in root)],\n                  key=lambda sample: sample[\"circuit\"]*100+sample[\"drawing\"]*10+sample[\"picture\"])\n\n\ndef read_image(sample: dict) -> np.ndarray:\n    \"\"\"Loads the Image Associated with a DB Sample\"\"\"\n\n    db_root = os.sep.join(realpath('/kaggle/input/cghd1152').split(os.sep))\n\n    return cv2.imread(join(db_root, f\"drafter_{sample['drafter']}\", \"images\", file_name(sample)))\n\n\ndef read_images(**kwargs) -> list:\n    \"\"\"Loads Images and BB Annotations and returns them as as List of Pairs\"\"\"\n\n    return [(read_image(sample), sample) for sample in read_dataset(**kwargs)]\n\n\n\ndef get_path(d: dict, ds_loc='/kaggle/input/cghd1152', mask=False):\n    if d[\"bboxes\"]:\n        name = file_name(d)\n        old_path = os.path.join(ds_loc, f\"drafter_{d['drafter']}\", 'images', name)\n    elif d['polygons']:\n        name = d['img_path'].split(\"/\")[-1]\n        if mask:\n            old_path = os.path.join(ds_loc, f\"drafter_{d['drafter']}\", 'segmentation', name)\n        else:\n            old_path = os.path.join(ds_loc, f\"drafter_{d['drafter']}\", 'images', name)\n    elif d['background']:\n        old_path = d['img_path']\n    else:\n        raise ValueError(\"Invalid sample dictionary format\")\n    return old_path\n\n\ndef sample_type(d: dict):\n    if len(d.get(\"bboxes\")) > 0:\n        return 'voc'\n    elif len(d.get(\"polygons\")) > 0:\n        return 'labelme'\n    elif d.get('background') == True:\n        return 'background'\n    else:\n        raise ValueError(\"Invalid sample dictionary format\")\n\n\ndef get_bboxes(d: dict, non_component_classes=[], components_only=False):\n    if d.get('bboxes'):\n        t = 'bboxes'\n        voc = True\n    elif d.get('polygons'):\n        t = 'polygons'\n        voc = False\n    elif d.get('background'):\n        return []\n    else:\n        raise ValueError(\"Invalid sample dictionary format\")\n\n    bboxes = d[t]\n\n    if not bboxes:\n        raise ValueError(\"Empty bounding boxes list in sample\")\n\n    if voc:\n        if not components_only:\n            return [bbox for bbox in bboxes]\n        else:\n            return [bbox for bbox in bboxes if bbox['class'] not in non_component_classes]\n    else:\n        l = []\n        for bbox in bboxes:\n            b = bbox['bbox']\n            b['class'] = bbox['class']\n            if not components_only or (components_only and b['class'] not in non_component_classes):\n                l.append(b)\n        return l\n\n\ndef filter_dataset(combined_ds, class2category, non_component_classes, reducing: set, unknown: set, deleting: set):\n    filtered_ds = deepcopy(combined_ds)\n    for i, sample in enumerate(filtered_ds):\n        if sample.get('bboxes') is not None:\n            t = 'bboxes'\n        elif sample.get('polygons') is not None:\n            t = 'polygons'\n        else:\n            continue  # Skip samples without bounding boxes\n\n        bboxes = sample[t]\n        bboxes = [bbox for bbox in bboxes if bbox['class'] not in deleting]\n        filtered_ds[i][t] = bboxes\n\n        for j, bbox in enumerate(bboxes):\n            if bbox['class'] in unknown:\n                bbox['class'] = 'unknown'\n            elif bbox['class'] in reducing:\n                bbox['class'] = bbox['class'].split('.')[0]\n\n    classes = set(class2category.keys()) - deleting - unknown - reducing\n    class2category = {key: i for i, key in enumerate(classes)}\n    instances_count, class_sorted_ds = count_instances(filtered_ds, class2category)\n    return filtered_ds, classes, class2category, instances_count, class_sorted_ds\n\n\ndef establish_dirs(path='/kaggle/working/yolo_dataset/'):\n    os.makedirs(os.path.join(path, 'val', 'images'), exist_ok=True)\n    os.makedirs(os.path.join(path, 'val', 'labels'), exist_ok=True)\n    os.makedirs(os.path.join(path, 'train', 'images'), exist_ok=True)\n    os.makedirs(os.path.join(path, 'train', 'labels'), exist_ok=True)\n    return True\n\n\ndef move_dataset_to(ds, filtered_dict, path='/kaggle/working/yolo_dataset/'):\n    for sample in ds:\n        sample_t = sample_type(sample)\n        if sample_t == 'background':\n            old_path = sample['img_path']\n            file_name = old_path.split('/')[-1]\n            new_path = os.path.join(path, 'train/images/', file_name)\n            shutil.copyfile(old_path, new_path)\n            with open(os.path.join(path, 'train/labels/', file_name.split('.')[0] + '.txt'), 'w') as file:\n                pass\n            continue\n\n        if sample_t in ('voc', 'labelme'):\n            old_path = get_path(sample)\n            name = old_path.split('/')[-1]\n        else:\n            raise ValueError(\"Invalid sample type\")\n\n        new_path = os.path.join(path, 'train/images/', name)\n        shutil.copyfile(old_path, new_path)\n        h, w = cv2.imread(new_path).shape[:2]\n\n        with open(os.path.join(path, 'train/labels/', name.split('.')[0] + '.txt'), 'w') as file:\n            pass  # This clears the file's contents\n\n        with open(os.path.join(path, 'train/labels/', name.split('.')[0] + '.txt'), 'a') as anno:\n            for bbox in sample.get('bboxes', []) if sample_t == 'voc' else sample.get('polygons', []):\n                if sample_t == 'labelme':\n                    b = bbox['bbox']\n                    xmin, ymin = int(b['xmin']), int(b['ymin'])\n                    xmax, ymax = int(b['xmax']), int(b['ymax'])\n                else:  # sample_t == 'voc'\n                    xmin, ymin = int(bbox['xmin']), int(bbox['ymin'])\n                    xmax, ymax = int(bbox['xmax']), int(bbox['ymax'])\n\n                x_center = ((xmin + xmax) / 2) / w\n                y_center = ((ymin + ymax) / 2) / h\n                width = (xmax - xmin) / w\n                height = (ymax - ymin) / h\n\n                class_index = filtered_dict.get(bbox['class'])\n                if class_index is None:\n                    print(f\"Warning: Class '{bbox['class']}' not found in filtered_dict. Skipping...\")\n                    continue\n\n                anno.write(f\"{class_index} {x_center} {y_center} {width} {height}\\n\")\n\n                if width > 1 or height > 1 or x_center > 1 or y_center > 1:\n                    print(f\"Error: Invalid bounding box values - \"\n                          f\"width: {width}, height: {height}, x_center: {x_center}, y_center: {y_center}\")\n                    print(f\"Sample width: {w}, height: {h}\")\n                    print(f\"Bounding box: {bbox}\")\n                    show_image_with_annotations(sample)\n                    raise ValueError(\"Invalid bounding box values detected\")\n\n\ndef get_mask(sample, ds_loc='/kaggle/input/cghd1152'):\n    if sample_type(sample) == 'labelme':\n        return cv2.imread(get_path(sample, ds_loc=ds_loc, mask=True), cv2.IMREAD_GRAYSCALE)\n    else:\n        return None\n\n\ndef get_filtered_mask(sample, non_component_classes=[]):\n    mask = get_mask(sample)\n    if mask is None:\n        return None\n\n    image_copy = mask.copy()  # Create a copy to avoid modifying the original mask\n\n    for bbox in get_bboxes(sample, non_component_classes):\n        if bbox['class'] in ('text', 'explanatory'):\n            image_copy[int(bbox['ymin']):int(bbox['ymax']), int(bbox['xmin']):int(bbox['xmax'])] = 255\n    return image_copy\n\n\ndef get_emptied_mask(sample, non_component_classes):\n    mask = get_mask(sample)\n    if mask is None:\n        return None\n\n    image_copy = mask.copy()  # Create a copy to avoid modifying the original mask\n\n    for bbox in get_bboxes(sample, non_component_classes):\n        if bbox['class'] not in ('crossover', 'junction', 'terminal'):\n            image_copy[int(bbox['ymin']):int(bbox['ymax']), int(bbox['xmin']):int(bbox['xmax'])] = 255\n    return image_copy\n\ndef show_image(img, title=\"Image\"):\n    plt.figure(figsize=(10, 8))\n    if len(img.shape) == 2:  # Grayscale image\n        plt.imshow(img, cmap='gray')\n    else:  # Color image\n        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n    plt.title(title)\n    plt.axis('off')\n    plt.show()\n\n# --- Helper function to check if a contour is fully inside a bbox ---\ndef is_contour_fully_inside(contour, bbox, thresh=0.2):\n    \"\"\"Checks if all points of a contour lie strictly within a bounding box.\"\"\"\n    xmin, ymin, xmax, ymax = bbox['xmin'], bbox['ymin'], bbox['xmax'], bbox['ymax']\n    # Ensure contour points are within the bbox boundaries\n    # Note: OpenCV contours are lists of points [[x1, y1]], [[x2, y2]], ...\n    xmargin = int(thresh * (xmax-xmin))\n    ymargin = int(thresh * (ymax-ymin))\n    for point in contour:\n        px, py = point[0] # Get the (x, y) coordinate\n        if not ((xmin-xmargin) <= px < (xmax+xmargin) and (ymin-ymargin) <= py < (ymax+ymargin)):\n            return False # Found a point outside the bbox\n    return True # All points are inside\n\n# --- Revised Mask Cleaning Function ---\ndef get_selectively_cleaned_mask(sample: dict,\n                                 classes_to_ignore: set = {'text', 'explanatory'},\n                                 ds_loc: str ='/kaggle/input/cghd1152'):\n    \"\"\"\n    Cleans a segmentation mask by removing only those contours that lie\n    entirely within the bounding boxes of specified classes to ignore.\n\n    Args:\n        sample (dict): The dataset sample dictionary (must be 'labelme' type).\n        classes_to_ignore (set): A set of class names whose bounding boxes\n                                 define regions where contained contours\n                                 should be removed.\n        ds_loc (str): The root location of the dataset.\n\n    Returns:\n        np.ndarray or None: The cleaned mask as a NumPy array, or None if\n                            the input sample is not 'labelme' type or has no mask.\n    \"\"\"\n    if sample_type(sample) != 'labelme':\n        print(\"Warning: Selective cleaning only applicable to 'labelme' samples with polygon data.\")\n        return None\n\n    mask = get_mask(sample, ds_loc=ds_loc)\n    if mask is None:\n        print(f\"Warning: No mask found for sample {sample_name(sample)}\")\n        return None\n\n    # --- Identify Bounding Boxes to Ignore ---\n    bboxes_to_ignore = []\n    if sample.get('polygons'): # Get bboxes from polygons\n        for poly in sample['polygons']:\n            if poly['class'] in classes_to_ignore:\n                # Ensure the bbox format is consistent (xmin, ymin, xmax, ymax)\n                bbox_data = poly.get('bbox')\n                if bbox_data and all(k in bbox_data for k in ['xmin', 'ymin', 'xmax', 'ymax']):\n                     bboxes_to_ignore.append(bbox_data)\n                else:\n                     # Calculate bbox if missing (though read_labelme should add it)\n                     points = poly.get('points', [])\n                     if points:\n                         xmin = min(p[0] for p in points)\n                         ymin = min(p[1] for p in points)\n                         xmax = max(p[0] for p in points)\n                         ymax = max(p[1] for p in points)\n                         bboxes_to_ignore.append({'xmin': xmin, 'ymin': ymin, 'xmax': xmax, 'ymax': ymax})\n\n    if not bboxes_to_ignore:\n        return mask # No regions to clean within, return original mask\n\n    # --- Prepare Mask for Contour Finding ---\n    # Assume mask uses 0 for object and 255 for background\n    # findContours needs white objects on black background\n    mask_binary = (mask < 127).astype(np.uint8) * 255 # Convert object pixels (0) to white (255)\n\n    # --- Find Contours ---\n    # Use RETR_LIST to get all contours without hierarchy complexity\n    contours, _ = cv2.findContours(mask_binary, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n\n    if not contours:\n        return mask # No segmented objects found, return original mask\n\n    # --- Create a Clean Copy and Erase Contained Contours ---\n    mask_cleaned = mask_binary.copy() # Start with the original mask\n    background_color = 0 # Assuming background is white (255)\n\n    contours_to_remove = []\n    for contour in contours:\n        # Check if this contour is fully inside ANY of the ignore boxes\n        is_fully_contained = False\n        for bbox_ignore in bboxes_to_ignore:\n            if is_contour_fully_inside(contour, bbox_ignore):\n                is_fully_contained = True\n                break # No need to check other ignore boxes for this contour\n\n        if is_fully_contained:\n            contours_to_remove.append(contour)\n\n    if contours_to_remove:\n        # Use -1 for thickness to fill the contours\n        cv2.drawContours(mask_cleaned, contours_to_remove, -1, color=background_color, thickness=cv2.FILLED)\n\n    return 255 - mask_cleaned","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T23:10:22.397687Z","iopub.execute_input":"2025-05-02T23:10:22.398035Z","iopub.status.idle":"2025-05-02T23:10:22.446503Z","shell.execute_reply.started":"2025-05-02T23:10:22.398017Z","shell.execute_reply":"2025-05-02T23:10:22.445695Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def show_image_and_mask(sample_dict, mean=None, std=None, title=\"Image and Mask\"):\n    \"\"\"\n    Displays the preprocessed image and its corresponding mask from the dataset sample.\n\n    Args:\n        sample_dict (dict): The dictionary returned by the Dataset's __getitem__.\n                            Expected keys: \"image\", \"mask\".\n        mean (list or tuple, optional): The mean used for normalization (e.g., SAM_IMAGE_MEAN).\n                                        Required to unnormalize the image for display.\n        std (list or tuple, optional): The standard deviation used for normalization (e.g., SAM_IMAGE_STD).\n                                       Required to unnormalize the image for display.\n        title (str): Title for the plot window.\n    \"\"\"\n    if \"image\" not in sample_dict or \"mask\" not in sample_dict:\n        print(\"Error: Sample dictionary must contain 'image' and 'mask' keys.\")\n        return\n\n    image_tensor = sample_dict[\"image\"] # Shape (C, H, W)\n    mask_tensor = sample_dict[\"mask\"]   # Shape (1, H, W)\n\n    # --- Input Validation ---\n    if not isinstance(image_tensor, torch.Tensor) or image_tensor.ndim != 3:\n        print(f\"Error: 'image' must be a 3D tensor (C, H, W), got {type(image_tensor)} with shape {image_tensor.shape}\")\n        return\n    if not isinstance(mask_tensor, torch.Tensor) or mask_tensor.ndim != 3 or mask_tensor.shape[0] != 1:\n        print(f\"Error: 'mask' must be a 3D tensor (1, H, W), got {type(mask_tensor)} with shape {mask_tensor.shape}\")\n        return\n    if image_tensor.shape[1:] != mask_tensor.shape[1:]:\n        print(f\"Error: Image shape {image_tensor.shape[1:]} and mask shape {mask_tensor.shape[1:]} mismatch.\")\n        return\n\n    # --- Unnormalize Image for Display ---\n    img_display = image_tensor.clone() # Avoid modifying the original tensor\n\n    if mean is not None and std is not None:\n        # Ensure mean and std are tensors with correct shape (C, 1, 1)\n        mean_t = torch.tensor(mean).view(img_display.shape[0], 1, 1)\n        std_t = torch.tensor(std).view(img_display.shape[0], 1, 1)\n\n        # Reverse normalization: (normalized * std) + mean\n        img_display.mul_(std_t).add_(mean_t)\n\n        # SAM normalization might have used 0-255 range before norm, clip to [0, 255]\n        img_display = torch.clamp(img_display, 0, 255)\n        # Convert to numpy HWC uint8 format for matplotlib\n        img_display_np = img_display.byte().permute(1, 2, 0).cpu().numpy()\n\n    else:\n        # If no mean/std provided, assume image tensor is in [0, 1] range\n        print(\"Warning: Mean/Std not provided. Assuming image tensor is in [0, 1] range for display.\")\n        img_display = torch.clamp(img_display, 0, 1)\n        # Convert to numpy HWC format\n        img_display_np = img_display.permute(1, 2, 0).cpu().numpy()\n        # If it wasn't [0,1], scale from [0, 255] if needed, but usually norm is applied\n        # img_display_np = (img_display_np * 255).astype(np.uint8)\n\n\n    # --- Prepare Mask for Display ---\n    # Remove channel dimension and convert to numpy\n    mask_display_np = mask_tensor.squeeze().cpu().numpy() # Shape (H, W)\n\n    # --- Plotting ---\n    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n    fig.suptitle(title, fontsize=14)\n\n    # Display Image\n    axes[0].imshow(img_display_np)\n    axes[0].set_title(\"Image (Unnormalized)\")\n    axes[0].axis('off')\n\n    # Display Mask\n    axes[1].imshow(mask_display_np, cmap='gray') # Use grayscale colormap\n    axes[1].set_title(\"Mask\")\n    axes[1].axis('off')\n\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T23:10:22.447886Z","iopub.execute_input":"2025-05-02T23:10:22.448106Z","iopub.status.idle":"2025-05-02T23:10:22.468945Z","shell.execute_reply.started":"2025-05-02T23:10:22.448090Z","shell.execute_reply":"2025-05-02T23:10:22.468262Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# loading the datset\nlabelme_ds = read_dataset(segmentation=True)\nprint(len(labelme_ds))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T23:10:22.469621Z","iopub.execute_input":"2025-05-02T23:10:22.469948Z","iopub.status.idle":"2025-05-02T23:10:26.292419Z","shell.execute_reply.started":"2025-05-02T23:10:22.469924Z","shell.execute_reply":"2025-05-02T23:10:26.291789Z"}},"outputs":[{"name":"stdout","text":"272\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## Showing example","metadata":{}},{"cell_type":"code","source":"# showing example\ncleaned_mask = get_selectively_cleaned_mask(labelme_ds[60], classes_to_ignore={'text', 'explanatory'})\nshow_image(cleaned_mask)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T23:10:26.293122Z","iopub.execute_input":"2025-05-02T23:10:26.293322Z","iopub.status.idle":"2025-05-02T23:10:26.474705Z","shell.execute_reply.started":"2025-05-02T23:10:26.293307Z","shell.execute_reply":"2025-05-02T23:10:26.473980Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x800 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":21},{"cell_type":"markdown","source":"## Saving datset locally","metadata":{}},{"cell_type":"code","source":"import os\n\nimages_dir = \"/kaggle/working/dataset/images\"\nmasks_dir =\"/kaggle/working/dataset/masks\"\nos.makedirs(images_dir, exist_ok = True)\nos.makedirs(masks_dir, exist_ok = True)\n\nfor i, d in enumerate(labelme_ds):\n    image = read_image(d)\n    mask = get_selectively_cleaned_mask(d, classes_to_ignore={'text', 'explanatory'})\n    #mask = get_mask(d)\n    try:\n        cv2.imwrite(os.path.join(images_dir, f\"{i}.jpg\"), image)\n        cv2.imwrite(os.path.join(masks_dir, f\"{i}.jpg\"), mask)\n    except:\n        print(\"problem with \", i)\n\nprint(len(os.listdir(images_dir)))\nprint(len(os.listdir(masks_dir)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T23:10:26.476460Z","iopub.execute_input":"2025-05-02T23:10:26.476738Z","iopub.status.idle":"2025-05-02T23:11:20.057080Z","shell.execute_reply.started":"2025-05-02T23:10:26.476710Z","shell.execute_reply":"2025-05-02T23:11:20.056353Z"}},"outputs":[{"name":"stderr","text":"[ WARN:0@848.611] global loadsave.cpp:268 findDecoder imread_('/kaggle/input/cghd1152/drafter_17/images/C196_D2_P4.jpg'): can't open/read file: check file path/integrity\n","output_type":"stream"},{"name":"stdout","text":"problem with  186\n","output_type":"stream"},{"name":"stderr","text":"[ WARN:0@849.222] global loadsave.cpp:268 findDecoder imread_('/kaggle/input/cghd1152/drafter_17/images/C198_D2_P1.jpg'): can't open/read file: check file path/integrity\n","output_type":"stream"},{"name":"stdout","text":"problem with  188\n","output_type":"stream"},{"name":"stderr","text":"[ WARN:0@850.316] global loadsave.cpp:268 findDecoder imread_('/kaggle/input/cghd1152/drafter_17/images/C202_D1_P2.jpg'): can't open/read file: check file path/integrity\n","output_type":"stream"},{"name":"stdout","text":"problem with  192\nproblem with  193\n","output_type":"stream"},{"name":"stderr","text":"[ WARN:0@850.644] global loadsave.cpp:268 findDecoder imread_('/kaggle/input/cghd1152/drafter_17/images/C203_D1_P3.jpg'): can't open/read file: check file path/integrity\n[ WARN:0@871.570] global loadsave.cpp:268 findDecoder imread_('/kaggle/input/cghd1152/drafter_23/images/C265_D1_P2.jpeg'): can't open/read file: check file path/integrity\n","output_type":"stream"},{"name":"stdout","text":"problem with  246\n267\n267\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"# Loading SAM 2","metadata":{}},{"cell_type":"markdown","source":"## Pre-definitions","metadata":{}},{"cell_type":"code","source":"import torch\nimport random\nimport torchvision\nimport os\nprint(\"PyTorch version:\", torch.__version__)\nprint(\"Torchvision version:\", torchvision.__version__)\nprint(\"CUDA is available:\", torch.cuda.is_available())\nimport sys\n\n# !sudo apt-get update\n# !sudo apt-get install python3-dev python3-pip build-essential\n#!{sys.executable} -m pip install --upgrade pip setuptools wheel\n!{sys.executable} -m pip install opencv-python matplotlib\n!{sys.executable} -m pip install 'git+https://github.com/facebookresearch/sam2.git'\n\n!mkdir -p ../checkpoints/\n!wget -P ../checkpoints/ https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T23:11:20.057840Z","iopub.execute_input":"2025-05-02T23:11:20.058440Z","iopub.status.idle":"2025-05-02T23:13:32.295214Z","shell.execute_reply.started":"2025-05-02T23:11:20.058420Z","shell.execute_reply":"2025-05-02T23:13:32.294141Z"}},"outputs":[{"name":"stdout","text":"PyTorch version: 2.5.1+cu124\nTorchvision version: 0.20.1+cu124\nCUDA is available: True\nRequirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.5)\nRequirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (1.26.4)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.2->opencv-python) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.2->opencv-python) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.2->opencv-python) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21.2->opencv-python) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21.2->opencv-python) (2024.2.0)\nCollecting git+https://github.com/facebookresearch/sam2.git\n  Cloning https://github.com/facebookresearch/sam2.git to /tmp/pip-req-build-xp66_yiy\n  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/sam2.git /tmp/pip-req-build-xp66_yiy\n  Resolved https://github.com/facebookresearch/sam2.git to commit 2b90b9f5ceec907a1c18123530e92e794ad901a4\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: torch>=2.5.1 in /usr/local/lib/python3.11/dist-packages (from SAM-2==1.0) (2.5.1+cu124)\nRequirement already satisfied: torchvision>=0.20.1 in /usr/local/lib/python3.11/dist-packages (from SAM-2==1.0) (0.20.1+cu124)\nRequirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from SAM-2==1.0) (1.26.4)\nRequirement already satisfied: tqdm>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from SAM-2==1.0) (4.67.1)\nRequirement already satisfied: hydra-core>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from SAM-2==1.0) (1.3.2)\nRequirement already satisfied: iopath>=0.1.10 in /usr/local/lib/python3.11/dist-packages (from SAM-2==1.0) (0.1.10)\nRequirement already satisfied: pillow>=9.4.0 in /usr/local/lib/python3.11/dist-packages (from SAM-2==1.0) (11.1.0)\nRequirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.11/dist-packages (from hydra-core>=1.3.2->SAM-2==1.0) (2.3.0)\nRequirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from hydra-core>=1.3.2->SAM-2==1.0) (4.9.3)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from hydra-core>=1.3.2->SAM-2==1.0) (24.2)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from iopath>=0.1.10->SAM-2==1.0) (4.13.1)\nRequirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from iopath>=0.1.10->SAM-2==1.0) (3.1.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.4->SAM-2==1.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.4->SAM-2==1.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.4->SAM-2==1.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.4->SAM-2==1.0) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.4->SAM-2==1.0) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.4->SAM-2==1.0) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (3.18.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.5.1->SAM-2==1.0) (1.3.0)\nRequirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from omegaconf<2.4,>=2.2->hydra-core>=1.3.2->SAM-2==1.0) (6.0.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.5.1->SAM-2==1.0) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.24.4->SAM-2==1.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.24.4->SAM-2==1.0) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.24.4->SAM-2==1.0) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.24.4->SAM-2==1.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.24.4->SAM-2==1.0) (2024.2.0)\n--2025-05-02 23:13:24--  https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt\nResolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.35.7.128, 13.35.7.82, 13.35.7.50, ...\nConnecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.35.7.128|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 898083611 (856M) [application/vnd.snesdev-page-table]\nSaving to: ‘../checkpoints/sam2.1_hiera_large.pt.1’\n\nsam2.1_hiera_large. 100%[===================>] 856.48M   131MB/s    in 7.4s    \n\n2025-05-02 23:13:32 (116 MB/s) - ‘../checkpoints/sam2.1_hiera_large.pt.1’ saved [898083611/898083611]\n\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import os\n# if using Apple MPS, fall back to CPU for unsupported ops\nos.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# select the device for computation\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelse:\n    device = torch.device(\"cpu\")\n    #device = torch.device(\"xla\")\nprint(f\"using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T23:13:32.296569Z","iopub.execute_input":"2025-05-02T23:13:32.296916Z","iopub.status.idle":"2025-05-02T23:13:32.303336Z","shell.execute_reply.started":"2025-05-02T23:13:32.296880Z","shell.execute_reply":"2025-05-02T23:13:32.302607Z"}},"outputs":[{"name":"stdout","text":"using device: cuda\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"## SAM 2 Instance","metadata":{}},{"cell_type":"code","source":"from sam2.build_sam import build_sam2\nfrom sam2.sam2_image_predictor import SAM2ImagePredictor\n\nsam2_checkpoint = \"../checkpoints/sam2.1_hiera_large.pt\"\nmodel_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n\nsam2_model = build_sam2(\n    model_cfg,\n    sam2_checkpoint,\n    device=device,\n    mode=\"train\"\n)\nsam2_model.use_high_res_features = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T23:13:32.304163Z","iopub.execute_input":"2025-05-02T23:13:32.304438Z","iopub.status.idle":"2025-05-02T23:13:35.223578Z","shell.execute_reply.started":"2025-05-02T23:13:32.304403Z","shell.execute_reply":"2025-05-02T23:13:35.223003Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"## SAM 2 Transforms","metadata":{}},{"cell_type":"code","source":"# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport warnings\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision.transforms import Normalize, Resize, ToTensor\n\n\nclass SAM2Transforms(nn.Module):\n    def __init__(\n        self, resolution, mask_threshold, max_hole_area=0.0, max_sprinkle_area=0.0\n    ):\n        \"\"\"\n        Transforms for SAM2.\n        \"\"\"\n        super().__init__()\n        self.resolution = resolution\n        self.mask_threshold = mask_threshold\n        self.max_hole_area = max_hole_area\n        self.max_sprinkle_area = max_sprinkle_area\n        self.mean = [0.485, 0.456, 0.406]\n        self.std = [0.229, 0.224, 0.225]\n        self.to_tensor = ToTensor()\n        self.transforms = nn.Sequential(\n                Resize((self.resolution, self.resolution)),\n                Normalize(self.mean, self.std),\n            )\n\n    def __call__(self, x):\n        x = self.to_tensor(x)\n        return self.transforms(x)\n\n    def forward_batch(self, img_list):\n        img_batch = [self.transforms(self.to_tensor(img)) for img in img_list]\n        img_batch = torch.stack(img_batch, dim=0)\n        return img_batch\n\n    def transform_coords(\n        self, coords: torch.Tensor, normalize=False, orig_hw=None\n    ) -> torch.Tensor:\n        \"\"\"\n        Expects a torch tensor with length 2 in the last dimension. The coordinates can be in absolute image or normalized coordinates,\n        If the coords are in absolute image coordinates, normalize should be set to True and original image size is required.\n\n        Returns\n            Un-normalized coordinates in the range of [0, 1] which is expected by the SAM2 model.\n        \"\"\"\n        if normalize:\n            assert orig_hw is not None\n            h, w = orig_hw\n            coords = coords.clone()\n            coords[..., 0] = coords[..., 0] / w\n            coords[..., 1] = coords[..., 1] / h\n\n        coords = coords * self.resolution  # unnormalize coords\n        return coords\n\n    def transform_boxes(\n        self, boxes: torch.Tensor, normalize=False, orig_hw=None\n    ) -> torch.Tensor:\n        \"\"\"\n        Expects a tensor of shape Bx4. The coordinates can be in absolute image or normalized coordinates,\n        if the coords are in absolute image coordinates, normalize should be set to True and original image size is required.\n        \"\"\"\n        boxes = self.transform_coords(boxes.reshape(-1, 2, 2), normalize, orig_hw)\n        return boxes\n\n    def postprocess_masks(self, masks: torch.Tensor, orig_hw) -> torch.Tensor:\n        \"\"\"\n        Perform PostProcessing on output masks.\n        \"\"\"\n        from sam2.utils.misc import get_connected_components\n\n        masks = masks.float()\n        input_masks = masks\n        mask_flat = masks.flatten(0, 1).unsqueeze(1)  # flatten as 1-channel image\n        try:\n            if self.max_hole_area > 0:\n                # Holes are those connected components in background with area <= self.fill_hole_area\n                # (background regions are those with mask scores <= self.mask_threshold)\n                labels, areas = get_connected_components(\n                    mask_flat <= self.mask_threshold\n                )\n                is_hole = (labels > 0) & (areas <= self.max_hole_area)\n                is_hole = is_hole.reshape_as(masks)\n                # We fill holes with a small positive mask score (10.0) to change them to foreground.\n                masks = torch.where(is_hole, self.mask_threshold + 10.0, masks)\n\n            if self.max_sprinkle_area > 0:\n                labels, areas = get_connected_components(\n                    mask_flat > self.mask_threshold\n                )\n                is_hole = (labels > 0) & (areas <= self.max_sprinkle_area)\n                is_hole = is_hole.reshape_as(masks)\n                # We fill holes with negative mask score (-10.0) to change them to background.\n                masks = torch.where(is_hole, self.mask_threshold - 10.0, masks)\n        except Exception as e:\n            # Skip the post-processing step if the CUDA kernel fails\n            warnings.warn(\n                f\"{e}\\n\\nSkipping the post-processing step due to the error above. You can \"\n                \"still use SAM 2 and it's OK to ignore the error above, although some post-processing \"\n                \"functionality may be limited (which doesn't affect the results in most cases; see \"\n                \"https://github.com/facebookresearch/sam2/blob/main/INSTALL.md).\",\n                category=UserWarning,\n                stacklevel=2,\n            )\n            masks = input_masks\n\n        masks = F.interpolate(masks, orig_hw, mode=\"bilinear\", align_corners=False)\n        return masks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T23:13:35.224322Z","iopub.execute_input":"2025-05-02T23:13:35.224515Z","iopub.status.idle":"2025-05-02T23:13:35.236393Z","shell.execute_reply.started":"2025-05-02T23:13:35.224501Z","shell.execute_reply":"2025-05-02T23:13:35.235771Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"_transforms = SAM2Transforms(\n            resolution=sam2_model.image_size,\n            mask_threshold=0,\n            max_hole_area=0,\n            max_sprinkle_area=0,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T23:13:35.237276Z","iopub.execute_input":"2025-05-02T23:13:35.237511Z","iopub.status.idle":"2025-05-02T23:13:35.552614Z","shell.execute_reply.started":"2025-05-02T23:13:35.237490Z","shell.execute_reply":"2025-05-02T23:13:35.551768Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"## Defining Wrapper","metadata":{}},{"cell_type":"code","source":"!pip install peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T23:13:35.553474Z","iopub.execute_input":"2025-05-02T23:13:35.553748Z","iopub.status.idle":"2025-05-02T23:13:38.793473Z","shell.execute_reply.started":"2025-05-02T23:13:35.553706Z","shell.execute_reply":"2025-05-02T23:13:38.792625Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (7.0.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.5.1+cu124)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.51.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.3.0)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.2)\nRequirement already satisfied: huggingface-hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.30.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2025.3.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->peft) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->peft) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2025.1.31)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->peft) (2024.2.0)\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"from sam2.modeling.sam2_base import SAM2Base\nfrom sam2.build_sam import build_sam2\nfrom sam2.sam2_image_predictor import SAM2ImagePredictor\nfrom peft import LoraConfig, get_peft_model, TaskType # Import necessary peft components\n\nclass MultiKernelRefinement(nn.Module):\n    \"\"\"\n    Applies multiple convolutional kernels in parallel for refinement\n    and combines their outputs.\n    \"\"\"\n    def __init__(self, in_channels=1, out_channels=1, kernel_sizes=[3, 5, 7, 9, 11], intermediate_channels=8):\n        \"\"\"\n        Args:\n            in_channels (int): Number of input channels (usually 1 for mask logits).\n            out_channels (int): Number of final output channels (usually 1 for refined logits).\n            kernel_sizes (list[int]): List of odd kernel sizes for parallel conv branches.\n            intermediate_channels (int): Number of output channels for EACH parallel conv branch.\n        \"\"\"\n        super().__init__()\n        self.kernel_sizes = kernel_sizes\n        self.intermediate_channels_per_branch = intermediate_channels\n\n        # Create parallel convolutional branches\n        self.conv_branches = nn.ModuleList()\n        for k_size in kernel_sizes:\n            # padding='same' ensures output H, W match input H, W for odd kernels\n            # For even kernels, padding needs manual calculation: padding = (k_size - 1) // 2\n            if k_size % 2 == 0:\n                 raise ValueError(f\"Even kernel size {k_size} not directly supported with padding='same'. Use odd kernels or calculate padding manually.\")\n            branch = nn.Conv2d(\n                in_channels=in_channels,\n                out_channels=self.intermediate_channels_per_branch,\n                kernel_size=k_size,\n                padding='same', # Works for odd kernel sizes\n                bias=True\n            )\n            self.conv_branches.append(branch)\n\n        # Activation function after each branch (optional but common)\n        self.activation = nn.GELU() # Or nn.GELU() etc.\n\n        # Final combination layer\n        # Takes concatenated features from all branches\n        total_intermediate_channels = len(kernel_sizes) * self.intermediate_channels_per_branch\n        self.combiner_conv = nn.Conv2d(\n            in_channels=total_intermediate_channels,\n            out_channels=out_channels,\n            kernel_size=1, # 1x1 convolution to combine features channel-wise\n            padding=0,\n            bias=True\n        )\n\n    def forward(self, x):\n        branch_outputs = []\n        for branch_conv in self.conv_branches:\n            branch_out = self.activation(branch_conv(x)) # Apply conv then activation\n            branch_outputs.append(branch_out)\n\n        # Concatenate outputs along the channel dimension\n        concatenated_features = torch.cat(branch_outputs, dim=1) # Shape: (B, total_intermediate_channels, H, W)\n\n        # Combine features using the 1x1 convolution\n        refined_output = self.combiner_conv(concatenated_features) # Shape: (B, out_channels, H, W)\n\n        return refined_output\n\nclass SAM2ImageWrapper(nn.Module):\n    \"\"\"\n    A wrapper around SAM2Base for image-only segmentation.\n    Applies LoRA internally to the wrapped model.\n    \"\"\"\n    def __init__(self, modified_sam2_model: SAM2Base, embedding_r=4, use_refinement=False, refinement_kernel_sizes=[3, 5, 7, 9, 11]):\n        super().__init__()\n        self.sam2_model = modified_sam2_model\n        self.use_refinement = use_refinement\n        self._bb_feat_sizes = [\n            (256, 256),\n            (128, 128),\n            (64, 64),\n        ]\n        \n        self.embedding_r = embedding_r\n        self.dense_embedding1 = nn.Parameter(torch.randn(1, 256, self.embedding_r))\n        self.dense_embedding2 = nn.Parameter(torch.randn(1, self.embedding_r, 64 * 64))\n        self.sparse_embedding = nn.Parameter(torch.randn(1, 32, 256))\n        if self.use_refinement:\n            self.refinement_layer = MultiKernelRefinement(\n                in_channels=1,\n                out_channels=1,\n                kernel_sizes=refinement_kernel_sizes, # Example kernel sizes\n                intermediate_channels=4 # Example intermediate channels per branch\n            )\n        else:\n            self.refinement_layer = None\n\n    def forward(self, images, points=None, point_labels=None, masks_prompt=None, multimask_output=False):\n        \"\"\"\n        Simplified forward pass using the wrapped SAM2Base's methods.\n        \"\"\"\n\n        # 1. Encode Image\n        out = self.sam2_model.image_encoder(images)\n        out[\"backbone_fpn\"][0] = self.sam2_model.sam_mask_decoder.conv_s0(\n            out[\"backbone_fpn\"][0]\n        )\n        out[\"backbone_fpn\"][1] = self.sam2_model.sam_mask_decoder.conv_s1(\n            out[\"backbone_fpn\"][1]\n        )\n\n        # 2. Prepare Decoder Inputs\n        _, vision_feats, _, _ = self.sam2_model._prepare_backbone_features(out)\n        # --- Corrected List Comprehension ---\n        feats = [\n            # Get Batch Size (B) dynamically from the input feature tensor\n            feat.permute(1, 2, 0).view(feat.shape[1], -1, *feat_size)\n            #                            ^^^^^^^^^^^  Use B from feat.shape[1]\n            for feat, feat_size in zip(vision_feats[::-1], self._bb_feat_sizes[::-1])\n        ][::-1] # Reverse the resulting list\n        # --- Dictionary Creation (remains the same) ---\n        _features = {\"image_embed\": feats[-1], \"high_res_feats\": feats[:-1]}\n\n        # 4. Run SAM Prompt Encoder and Mask Decoder directly\n        high_res_features = _features[\"high_res_feats\"]\n\n        # compute the trainable prompt embedding\n        dense_embedding = (self.dense_embedding1 @ self.dense_embedding2).view(1, 256, 64, 64)\n        \n        low_res_masks, iou_predictions, _, _ = self.sam2_model.sam_mask_decoder(\n            image_embeddings=_features[\"image_embed\"],\n            image_pe=self.sam2_model.sam_prompt_encoder.get_dense_pe(),\n            sparse_prompt_embeddings=self.sparse_embedding,\n            dense_prompt_embeddings=dense_embedding,\n            multimask_output=False,\n            repeat_image=True,\n            high_res_features=high_res_features,\n        )\n\n        # 5. Return desired outputs\n        high_res_masks = F.interpolate(\n            low_res_masks,\n            size=(self.sam2_model.image_size, self.sam2_model.image_size),\n            mode=\"bilinear\",\n            align_corners=False,\n        )\n\n\n        if self.use_refinement:\n            high_res_masks = self.refinement_layer(high_res_masks)\n            \n        \n        return high_res_masks, low_res_masks, iou_predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T23:13:38.794695Z","iopub.execute_input":"2025-05-02T23:13:38.795299Z","iopub.status.idle":"2025-05-02T23:13:38.811638Z","shell.execute_reply.started":"2025-05-02T23:13:38.795273Z","shell.execute_reply":"2025-05-02T23:13:38.810794Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"def get_modified_sam2(\n    # --- Model Config ---\n    model_cfg_path: str = \"configs/sam2.1/sam2.1_hiera_l.yaml\",\n    checkpoint_path: str = \"../checkpoints/sam2.1_hiera_large.pt\",\n    device: str = \"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n    use_high_res_features: bool = True,\n    # --- PEFT Config ---\n    use_peft: bool = True,\n    lora_rank: int = 12,\n    lora_alpha: int = 16,\n    lora_dropout: float = 0.2,\n    lora_target_modules: list = None, # List of module names (strings)\n    # --- Wrapper/Task Config ---\n    use_wrapper: bool = True,\n    trainable_embedding_r: int = 4,\n    # --- Refinement Layer ---\n    use_refinement_layer: bool = False,\n    refinement_kernels: list = [3, 5, 7, 11],\n    kernel_channels: int = 4,\n    # --- Loss Settings ---\n    weight_dice=0.5, weight_focal=0.4, weight_iou=0.1, \n    weight_tversky: float = 0.0, weight_tv: float = 0.0, weight_freq: float = 0.0,\n    dice_smooth=1e-5, focal_alpha=0.25, focal_gamma=2.0,\n    iou_smooth=1e-5, iou_threshold=0.5,\n    tversky_alpha = 0.2, tversky_beta= 0.8,\n    apply_sigmoid=True,\n    # --- Optimizer Settings ---\n    lr=1e-3\n    ):\n    \"\"\"\n    Initializes SAM 2, applies PEFT/LoRA, and optionally wraps it for image-only tasks.\n\n    Args:\n        model_cfg_path (str): Path to the SAM 2 model config YAML.\n        checkpoint_path (str): Path to the SAM 2 model checkpoint (.pt file).\n        device (str): Device to load the model onto ('cuda', 'cpu', 'mps').\n        use_high_res_features (bool): Whether the decoder should use high-res skip connections.\n        use_peft (bool): Whether to apply PEFT/LoRA.\n        lora_rank (int): Rank for LoRA matrices.\n        lora_alpha (int): Alpha scaling for LoRA.\n        lora_dropout (float): Dropout probability for LoRA layers.\n        lora_target_modules (list): List of specific module names (strings) within the original\n                                     SAM2 model to apply LoRA to. If None, PEFT might guess or\n                                     you might need to define defaults.\n        use_wrapper (bool): Whether to wrap the (PEFT-)modified model in SAM2ImageWrapper.\n        trainable_embedding_r (int): Rank factor for the trainable prompt embeddings in the wrapper.\n        use_refinement_layer (bool): Whether to add the MultiKernelRefinement layer in the wrapper.\n\n    Returns:\n        torch.nn.Module: The potentially PEFT-modified and wrapped SAM 2 model.\n    \"\"\"\n    print(\"--- Initializing Modified SAM 2 ---\")\n    model_device = torch.device(device)\n\n    # 1. Load Original SAM 2 Model\n    print(f\"Loading SAM 2 from config: {model_cfg_path} and checkpoint: {checkpoint_path}\")\n    original_sam2_model = build_sam2(\n        model_cfg_path,\n        checkpoint_path,\n        device=model_device,\n        mode=\"train\" # Keep in train mode for fine-tuning\n    )\n    original_sam2_model.use_high_res_features_in_sam = use_high_res_features\n    print(f\"Original model loaded on {model_device}. use_high_res_features_in_sam set to {use_high_res_features}.\")\n\n    # --- Model to be returned ---\n    final_model = original_sam2_model\n\n    # 2. Apply PEFT/LoRA if enabled\n    if use_peft:\n        print(f\"Applying PEFT/LoRA with rank={lora_rank}, alpha={lora_alpha}\")\n        if lora_target_modules is None:\n            # Define default target modules if none provided\n            # These defaults should cover key areas for image-only fine-tuning\n            lora_target_modules = [\n                # Decoder Transformer Attention (Self and Cross)\n                \"sam_mask_decoder.transformer.layers.0.self_attn.k_proj\",\n            ]\n            print(f\"Using default lora_target_modules: {lora_target_modules}\")\n\n        lora_config = LoraConfig(\n            r=lora_rank,\n            lora_alpha=lora_alpha,\n            target_modules=lora_target_modules,\n            lora_dropout=lora_dropout,\n            bias=\"none\", # Common setting\n            modules_to_save=None, # Only train LoRA parameters\n            init_lora_weights=True, # Default initialization\n        )\n\n        # Apply PEFT\n        # Note: get_peft_model freezes non-target layers automatically\n        peft_model = get_peft_model(original_sam2_model, lora_config)\n        print(\"PEFT LoRA configuration applied.\")\n        peft_model.print_trainable_parameters()\n        final_model = peft_model # Update the model to be returned\n    else:\n        print(\"Skipping PEFT/LoRA application. Freezing all parameters.\")\n        # Freeze all parameters if not using PEFT, as wrapper adds new ones\n        for param in final_model.parameters():\n             param.requires_grad = False\n            \n    # 3. Apply Wrapper if enabled\n    if use_wrapper:\n        print(\"Applying SAM2ImageWrapper...\")\n        wrapped_model = SAM2ImageWrapper(\n            modified_sam2_model=final_model, # Pass the (potentially PEFT-modified) model\n            embedding_r=trainable_embedding_r,\n            use_refinement=use_refinement_layer,\n            refinement_kernel_sizes=refinement_kernels\n        )\n        final_model = wrapped_model.to(model_device) # Update the model to be returned\n        print(\"Wrapper applied.\")\n    else:\n        print(\"Skipping SAM2ImageWrapper.\")\n        # If not using the wrapper, ensure the training loop correctly calls\n        # the PEFT model with the image-only logic and handles prompts.\n\n    # 4. Final Verification of Trainable Parameters\n    print(\"\\n--- Final Trainable Parameters ---\")\n    total_trainable = 0\n    for name, param in final_model.named_parameters():\n        if param.requires_grad:\n            print(f\"- {name}: {param.shape} ({param.numel()})\")\n            total_trainable += param.numel()\n    print(f\"Total Trainable Parameters in Final Model: {total_trainable}\")\n    if total_trainable == 0 and (use_peft or use_wrapper):\n         print(\"WARNING: No trainable parameters found! Check PEFT config and wrapper parameter initialization.\")\n    elif not use_peft and not use_wrapper and total_trainable > 0:\n         print(\"Warning: Model has trainable parameters but PEFT/Wrapper were not used?\")\n\n\n    print(\"--- Optimizer Is Ready ---\")\n    return final_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T23:13:38.812666Z","iopub.execute_input":"2025-05-02T23:13:38.812947Z","iopub.status.idle":"2025-05-02T23:13:38.830369Z","shell.execute_reply.started":"2025-05-02T23:13:38.812925Z","shell.execute_reply":"2025-05-02T23:13:38.829583Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"base_parts = [\"sam_mask_decoder.transformer.layers.0.self_attn.k_proj\",\n                \"sam_mask_decoder.transformer.layers.0.self_attn.q_proj\",\n                \"sam_mask_decoder.transformer.layers.0.self_attn.v_proj\",\n                \"sam_mask_decoder.transformer.layers.0.self_attn.out_proj\",\n                \"sam_mask_decoder.transformer.layers.1.self_attn.k_proj\",\n                \"sam_mask_decoder.transformer.layers.1.self_attn.q_proj\",\n                \"sam_mask_decoder.transformer.layers.1.self_attn.v_proj\",\n                \"sam_mask_decoder.transformer.layers.1.self_attn.out_proj\",\n                \"sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj\",\n                \"sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj\",\n                \"sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj\",\n                \"sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj\",\n                \"sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj\",\n                \"sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj\",\n                \"sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj\",\n                \"sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj\",\n                \"sam_mask_decoder.transformer.layers.0.mlp.layers.0\",\n                \"sam_mask_decoder.transformer.layers.0.mlp.layers.1\",\n                \"sam_mask_decoder.transformer.layers.1.mlp.layers.0\",\n                \"sam_mask_decoder.transformer.layers.1.mlp.layers.1\"]\nadded_parts = [\n            \"sam_mask_decoder.iou_prediction_head.layers.2\",\n             \"sam_mask_decoder.conv_s0\",\n                \"sam_mask_decoder.conv_s1\",\n             \n             \"image_encoder.neck.convs.2.conv\",\n            \"image_encoder.neck.convs.3.conv\", \n             \n             \"image_encoder.trunk.blocks.44.attn.qkv\", # for downsampling\n              #\"image_encoder.trunk.blocks.44.attn.proj\",\n              \"image_encoder.trunk.blocks.44.mlp.layers.0\",\n              #\"image_encoder.trunk.blocks.44.mlp.layers.1\",\n              \"image_encoder.trunk.blocks.44.proj\",\n\n            \"image_encoder.trunk.blocks.47.attn.qkv\",\n            #\"image_encoder.trunk.blocks.47.attn.proj\",\n            \"image_encoder.trunk.blocks.47.mlp.layers.0\",\n            #\"image_encoder.trunk.blocks.47.mlp.layers.1\", # least needed\n            \n            \"sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj\",\n            \"sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj\",\n            \"sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj\",\n            #\"sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj\",\n            \"sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj\",\n            \"sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj\",\n            \"sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj\",]\n            #\"sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T23:13:38.833377Z","iopub.execute_input":"2025-05-02T23:13:38.833597Z","iopub.status.idle":"2025-05-02T23:13:38.848660Z","shell.execute_reply.started":"2025-05-02T23:13:38.833582Z","shell.execute_reply":"2025-05-02T23:13:38.847869Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"modified_sam2 = get_modified_sam2(\n    lora_rank = 4,\n    lora_alpha = 16,\n    weight_dice= 0.5, weight_focal=0.4, weight_iou=0.3, weight_freq=0.1,\n    use_refinement_layer = True,\n    refinement_kernels = [3, 5, 7, 11],\n    kernel_channels = 2,\n    focal_alpha=0.25,\n    lora_target_modules= base_parts + added_parts,\n    lora_dropout = 0.3,\n    lr=1e-3\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T23:13:38.849436Z","iopub.execute_input":"2025-05-02T23:13:38.849661Z","iopub.status.idle":"2025-05-02T23:13:41.970169Z","shell.execute_reply.started":"2025-05-02T23:13:38.849646Z","shell.execute_reply":"2025-05-02T23:13:41.969392Z"}},"outputs":[{"name":"stdout","text":"--- Initializing Modified SAM 2 ---\nLoading SAM 2 from config: configs/sam2.1/sam2.1_hiera_l.yaml and checkpoint: ../checkpoints/sam2.1_hiera_large.pt\nOriginal model loaded on cuda:0. use_high_res_features_in_sam set to True.\nApplying PEFT/LoRA with rank=4, alpha=16\nPEFT LoRA configuration applied.\ntrainable params: 169,552 || all params: 224,616,194 || trainable%: 0.0755\nApplying SAM2ImageWrapper...\nWrapper applied.\n\n--- Final Trainable Parameters ---\n- dense_embedding1: torch.Size([1, 256, 4]) (1024)\n- dense_embedding2: torch.Size([1, 4, 4096]) (16384)\n- sparse_embedding: torch.Size([1, 32, 256]) (8192)\n- sam2_model.base_model.model.image_encoder.trunk.blocks.44.attn.qkv.lora_A.default.weight: torch.Size([4, 576]) (2304)\n- sam2_model.base_model.model.image_encoder.trunk.blocks.44.attn.qkv.lora_B.default.weight: torch.Size([3456, 4]) (13824)\n- sam2_model.base_model.model.image_encoder.trunk.blocks.44.mlp.layers.0.lora_A.default.weight: torch.Size([4, 1152]) (4608)\n- sam2_model.base_model.model.image_encoder.trunk.blocks.44.mlp.layers.0.lora_B.default.weight: torch.Size([4608, 4]) (18432)\n- sam2_model.base_model.model.image_encoder.trunk.blocks.44.proj.lora_A.default.weight: torch.Size([4, 576]) (2304)\n- sam2_model.base_model.model.image_encoder.trunk.blocks.44.proj.lora_B.default.weight: torch.Size([1152, 4]) (4608)\n- sam2_model.base_model.model.image_encoder.trunk.blocks.47.attn.qkv.lora_A.default.weight: torch.Size([4, 1152]) (4608)\n- sam2_model.base_model.model.image_encoder.trunk.blocks.47.attn.qkv.lora_B.default.weight: torch.Size([3456, 4]) (13824)\n- sam2_model.base_model.model.image_encoder.trunk.blocks.47.mlp.layers.0.lora_A.default.weight: torch.Size([4, 1152]) (4608)\n- sam2_model.base_model.model.image_encoder.trunk.blocks.47.mlp.layers.0.lora_B.default.weight: torch.Size([4608, 4]) (18432)\n- sam2_model.base_model.model.image_encoder.neck.convs.2.conv.lora_A.default.weight: torch.Size([4, 288, 1, 1]) (1152)\n- sam2_model.base_model.model.image_encoder.neck.convs.2.conv.lora_B.default.weight: torch.Size([256, 4, 1, 1]) (1024)\n- sam2_model.base_model.model.image_encoder.neck.convs.3.conv.lora_A.default.weight: torch.Size([4, 144, 1, 1]) (576)\n- sam2_model.base_model.model.image_encoder.neck.convs.3.conv.lora_B.default.weight: torch.Size([256, 4, 1, 1]) (1024)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.0.self_attn.q_proj.lora_A.default.weight: torch.Size([4, 256]) (1024)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.0.self_attn.q_proj.lora_B.default.weight: torch.Size([256, 4]) (1024)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.0.self_attn.k_proj.lora_A.default.weight: torch.Size([4, 256]) (1024)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.0.self_attn.k_proj.lora_B.default.weight: torch.Size([256, 4]) (1024)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.0.self_attn.v_proj.lora_A.default.weight: torch.Size([4, 256]) (1024)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.0.self_attn.v_proj.lora_B.default.weight: torch.Size([256, 4]) (1024)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.0.self_attn.out_proj.lora_A.default.weight: torch.Size([4, 256]) (1024)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.0.self_attn.out_proj.lora_B.default.weight: torch.Size([256, 4]) (1024)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.lora_A.default.weight: torch.Size([4, 256]) (1024)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.lora_B.default.weight: torch.Size([128, 4]) (512)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.lora_A.default.weight: torch.Size([4, 256]) (1024)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.lora_B.default.weight: torch.Size([128, 4]) (512)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.lora_A.default.weight: torch.Size([4, 256]) (1024)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.lora_B.default.weight: torch.Size([128, 4]) (512)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.lora_A.default.weight: torch.Size([4, 128]) (512)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.lora_B.default.weight: torch.Size([256, 4]) (1024)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.0.mlp.layers.0.lora_A.default.weight: torch.Size([4, 256]) (1024)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.0.mlp.layers.0.lora_B.default.weight: torch.Size([2048, 4]) (8192)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.0.mlp.layers.1.lora_A.default.weight: torch.Size([4, 2048]) (8192)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.0.mlp.layers.1.lora_B.default.weight: torch.Size([256, 4]) (1024)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.lora_A.default.weight: torch.Size([4, 256]) (1024)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.lora_B.default.weight: torch.Size([128, 4]) (512)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.lora_A.default.weight: torch.Size([4, 256]) (1024)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.lora_B.default.weight: torch.Size([128, 4]) (512)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.lora_A.default.weight: torch.Size([4, 256]) (1024)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.lora_B.default.weight: torch.Size([128, 4]) (512)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.1.self_attn.q_proj.lora_A.default.weight: torch.Size([4, 256]) (1024)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.1.self_attn.q_proj.lora_B.default.weight: torch.Size([256, 4]) (1024)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.1.self_attn.k_proj.lora_A.default.weight: torch.Size([4, 256]) (1024)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.1.self_attn.k_proj.lora_B.default.weight: torch.Size([256, 4]) (1024)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.1.self_attn.v_proj.lora_A.default.weight: torch.Size([4, 256]) (1024)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.1.self_attn.v_proj.lora_B.default.weight: torch.Size([256, 4]) (1024)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.1.self_attn.out_proj.lora_A.default.weight: torch.Size([4, 256]) (1024)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.1.self_attn.out_proj.lora_B.default.weight: torch.Size([256, 4]) (1024)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.lora_A.default.weight: torch.Size([4, 256]) (1024)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.lora_B.default.weight: torch.Size([128, 4]) (512)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.lora_A.default.weight: torch.Size([4, 256]) (1024)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.lora_B.default.weight: torch.Size([128, 4]) (512)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.lora_A.default.weight: torch.Size([4, 256]) (1024)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.lora_B.default.weight: torch.Size([128, 4]) (512)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.lora_A.default.weight: torch.Size([4, 128]) (512)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.lora_B.default.weight: torch.Size([256, 4]) (1024)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.1.mlp.layers.0.lora_A.default.weight: torch.Size([4, 256]) (1024)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.1.mlp.layers.0.lora_B.default.weight: torch.Size([2048, 4]) (8192)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.1.mlp.layers.1.lora_A.default.weight: torch.Size([4, 2048]) (8192)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.1.mlp.layers.1.lora_B.default.weight: torch.Size([256, 4]) (1024)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.lora_A.default.weight: torch.Size([4, 256]) (1024)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.lora_B.default.weight: torch.Size([128, 4]) (512)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.lora_A.default.weight: torch.Size([4, 256]) (1024)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.lora_B.default.weight: torch.Size([128, 4]) (512)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.lora_A.default.weight: torch.Size([4, 256]) (1024)\n- sam2_model.base_model.model.sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.lora_B.default.weight: torch.Size([128, 4]) (512)\n- sam2_model.base_model.model.sam_mask_decoder.conv_s0.lora_A.default.weight: torch.Size([4, 256, 1, 1]) (1024)\n- sam2_model.base_model.model.sam_mask_decoder.conv_s0.lora_B.default.weight: torch.Size([32, 4, 1, 1]) (128)\n- sam2_model.base_model.model.sam_mask_decoder.conv_s1.lora_A.default.weight: torch.Size([4, 256, 1, 1]) (1024)\n- sam2_model.base_model.model.sam_mask_decoder.conv_s1.lora_B.default.weight: torch.Size([64, 4, 1, 1]) (256)\n- sam2_model.base_model.model.sam_mask_decoder.iou_prediction_head.layers.2.lora_A.default.weight: torch.Size([4, 256]) (1024)\n- sam2_model.base_model.model.sam_mask_decoder.iou_prediction_head.layers.2.lora_B.default.weight: torch.Size([4, 4]) (16)\n- refinement_layer.conv_branches.0.weight: torch.Size([4, 1, 3, 3]) (36)\n- refinement_layer.conv_branches.0.bias: torch.Size([4]) (4)\n- refinement_layer.conv_branches.1.weight: torch.Size([4, 1, 5, 5]) (100)\n- refinement_layer.conv_branches.1.bias: torch.Size([4]) (4)\n- refinement_layer.conv_branches.2.weight: torch.Size([4, 1, 7, 7]) (196)\n- refinement_layer.conv_branches.2.bias: torch.Size([4]) (4)\n- refinement_layer.conv_branches.3.weight: torch.Size([4, 1, 11, 11]) (484)\n- refinement_layer.conv_branches.3.bias: torch.Size([4]) (4)\n- refinement_layer.combiner_conv.weight: torch.Size([1, 16, 1, 1]) (16)\n- refinement_layer.combiner_conv.bias: torch.Size([1]) (1)\nTotal Trainable Parameters in Final Model: 196001\n--- Optimizer Is Ready ---\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"## Using checkpoint","metadata":{}},{"cell_type":"code","source":"!wget -O miou_last.pth --no-check-certificate \"https://storage.googleapis.com/kaggle-script-versions/237121567/output/logs/semi_learning_base/run_semi_iouw_20250430_233309/best_miou_model.pth?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20250502%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250502T104725Z&X-Goog-Expires=3600&X-Goog-SignedHeaders=host&X-Goog-Signature=74de16cbc84837dae238050504c3af36264092515680353fd604abb2423370cef41e271fcba60082de629e217d3a6810de019c684e6cbef965ed3b23fefa26c6b5ec7af5c8edf1b89a21f6ad6d8bac03162ec600ff4c1013f8e539d218e68cc80c6e38f9b3f1aab2d270d20817c27ac0b508f0f2eaf40b4bf1f2841535961707c5a3129b4fdb5d24dba90580f9b249bd0a88f76f290dfe8db40e324ee2fb0b38f3a177ce50c3162acfe9a9765980b5ba7078681c60f7c532716d1fc794e767ce85bae367f1e2077af78086a407f9a30d3480309ae40cdfc89fd69af93fef793417db1a8cbf73730b298d7056df8353ad5431c749b66aeabdc88717aa6eda69fb\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T23:13:41.971353Z","iopub.execute_input":"2025-05-02T23:13:41.971585Z","iopub.status.idle":"2025-05-02T23:13:43.055614Z","shell.execute_reply.started":"2025-05-02T23:13:41.971568Z","shell.execute_reply":"2025-05-02T23:13:43.054848Z"}},"outputs":[{"name":"stdout","text":"--2025-05-02 23:13:42--  https://storage.googleapis.com/kaggle-script-versions/237121567/output/logs/semi_learning_base/run_semi_iouw_20250430_233309/best_miou_model.pth?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20250502%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250502T104725Z&X-Goog-Expires=3600&X-Goog-SignedHeaders=host&X-Goog-Signature=74de16cbc84837dae238050504c3af36264092515680353fd604abb2423370cef41e271fcba60082de629e217d3a6810de019c684e6cbef965ed3b23fefa26c6b5ec7af5c8edf1b89a21f6ad6d8bac03162ec600ff4c1013f8e539d218e68cc80c6e38f9b3f1aab2d270d20817c27ac0b508f0f2eaf40b4bf1f2841535961707c5a3129b4fdb5d24dba90580f9b249bd0a88f76f290dfe8db40e324ee2fb0b38f3a177ce50c3162acfe9a9765980b5ba7078681c60f7c532716d1fc794e767ce85bae367f1e2077af78086a407f9a30d3480309ae40cdfc89fd69af93fef793417db1a8cbf73730b298d7056df8353ad5431c749b66aeabdc88717aa6eda69fb\nResolving storage.googleapis.com (storage.googleapis.com)... 142.250.157.207, 64.233.187.207, 142.251.8.207, ...\nConnecting to storage.googleapis.com (storage.googleapis.com)|142.250.157.207|:443... connected.\nHTTP request sent, awaiting response... 400 Bad Request\n2025-05-02 23:13:42 ERROR 400: Bad Request.\n\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"ckp_path = '/kaggle/working/miou_last.pth'\ncheckpoint = torch.load(ckp_path, map_location=device, weights_only=True) \nmodel_state_dict = checkpoint['state_dict'] \nmodified_sam2.load_state_dict(model_state_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T23:13:43.056679Z","iopub.execute_input":"2025-05-02T23:13:43.057017Z","iopub.status.idle":"2025-05-02T23:13:45.200076Z","shell.execute_reply.started":"2025-05-02T23:13:43.056981Z","shell.execute_reply":"2025-05-02T23:13:45.199169Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/574789066.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mckp_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/kaggle/working/miou_last.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckp_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel_state_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodified_sam2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_state_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1374\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m                 return _legacy_load(\n\u001b[0m\u001b[1;32m   1377\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m                     \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1626\u001b[0m         )\n\u001b[1;32m   1627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1628\u001b[0;31m     \u001b[0mmagic_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1629\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmagic_number\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mMAGIC_NUMBER\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid magic number; corrupt file?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_weights_only_unpickler.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, encoding)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ASCII\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_weights_only_unpickler.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;31m# Risky operators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mEOFError\u001b[0m: "],"ename":"EOFError","evalue":"","output_type":"error"}],"execution_count":34},{"cell_type":"markdown","source":"# Visualization","metadata":{}},{"cell_type":"markdown","source":"## One example","metadata":{}},{"cell_type":"code","source":"test_img = \"/kaggle/working/dataset/images/40.jpg\"\nimage_orig = Image.open(test_img).convert(\"RGB\")\nimage = np.array(image_orig)\n_orig_hw = [image.shape[:2]]\n_orig_hw","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_rs = _transforms(image).unsqueeze(0)\nhigh_res_mask, low_res_mask, _ = modified_sam2(image_rs.to(device))\nmask_filtered = _transforms.postprocess_masks(\n            high_res_mask, _orig_hw[0])\nmask_unfiltered = _transforms.postprocess_masks(\n            low_res_mask, _orig_hw[0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_image_and_mask({\"image\":_transforms.to_tensor(image),\"mask\":mask_unfiltered[0].detach()>0})\nshow_image_and_mask({\"image\":_transforms.to_tensor(image),\"mask\":mask_filtered[0].detach()>0})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Frequency example","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os # For saving plot\n\ndef get_log_spectrum(input_map: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Calculates the log-magnitude of the 2D FFT spectrum, shifted for visualization.\n\n    Args:\n        input_map (torch.Tensor): Input 2D map (H, W) on any device.\n\n    Returns:\n        torch.Tensor: Shifted log-magnitude spectrum (H, W) on the same device.\n    \"\"\"\n    if input_map.dim() != 2:\n        raise ValueError(f\"Input map must be 2D (H, W), got shape {input_map.shape}\")\n\n    # 1. Apply 2D FFT\n    # fft2 computes the DFT across the last two dimensions by default\n    fft_map = torch.fft.fft2(input_map)\n\n    # 2. Shift the zero frequency component to the center\n    fft_map_shifted = torch.fft.fftshift(fft_map)\n\n    # 3. Calculate Magnitude\n    magnitude_spectrum = torch.abs(fft_map_shifted)\n\n    # 4. Apply Logarithm for better visualization (log(1+x) for stability)\n    log_magnitude_spectrum = torch.log1p(magnitude_spectrum)\n\n    return log_magnitude_spectrum\n\ndef visualize_frequency_spectrum(\n    gt_mask: torch.Tensor,\n    pred_prob: torch.Tensor,\n    sample_idx: int = 0,\n    original_image: torch.Tensor = None, # Optional original image\n    save_path: str = None,\n    title_suffix: str = \"\"\n):\n    # --- Input Preparation ---\n    # Select the sample and squeeze channel dimension if present\n    # --- FIX: Add .detach() before .cpu() ---\n    gt_sample_detached = gt_mask[sample_idx].squeeze().detach()\n    pred_sample_detached = pred_prob[sample_idx].squeeze().detach()\n    \n    target_device = pred_sample_detached.device\n    gt_sample = gt_sample_detached.to(target_device) # Move gt_sample if necessary\n    pred_sample = pred_sample_detached # Already on target device (or move if needed))\n\n    # Ensure they are 2D\n    if gt_sample.dim() != 2 or pred_sample.dim() != 2:\n        raise ValueError(\"Ground truth and prediction must be effectively 2D after sample selection.\")\n\n    # Device for calculation\n    device = gt_sample.device # Can stay on original device\n\n    # --- Calculate Spectra ---\n    # No grad context is good here\n    with torch.no_grad():\n        log_spectrum_gt = get_log_spectrum(gt_sample.float())\n        log_spectrum_pred = get_log_spectrum(pred_sample.float())\n        spectrum_diff = torch.abs(log_spectrum_pred - log_spectrum_gt)\n\n    # --- Prepare for Plotting (Move to CPU, convert to NumPy) ---\n    # --- FIX: Already detached, just call .cpu().numpy() ---\n    log_spectrum_gt_np = log_spectrum_gt.cpu().numpy()\n    log_spectrum_pred_np = log_spectrum_pred.cpu().numpy()\n    spectrum_diff_np = spectrum_diff.cpu().numpy()\n\n    # Prepare original masks/image for plotting\n    # --- FIX: Use the detached tensors ---\n    gt_plot = gt_sample.cpu().numpy()\n    pred_plot = pred_sample.cpu().numpy()\n\n    has_original_image = original_image is not None\n    if has_original_image:\n        # --- FIX: Detach original image too ---\n        img_plot = original_image[sample_idx].detach().cpu().numpy()\n        # Handle image format (assume CHW -> HWC for plotting if C=3)\n        if img_plot.shape[0] == 3:\n             img_plot = np.transpose(img_plot, (1, 2, 0))\n             # --- Add your denormalization here if needed ---\n        elif img_plot.shape[0] == 1:\n            img_plot = img_plot.squeeze()\n\n    # --- Create Plot ---\n    # --- FIX: Correct calculation of num_cols ---\n    num_cols = 6 if has_original_image else 5 # 6 plots with image, 5 without\n    fig, axes = plt.subplots(1, num_cols, figsize=(num_cols * 5, 5))\n    # Ensure axes is always treated as an array, even if num_cols=1 (though unlikely here)\n    if num_cols == 1:\n        axes = [axes]\n    #-------------------------------------------\n\n    fig.suptitle(f\"Frequency Spectrum Comparison {title_suffix} (Sample {sample_idx})\", fontsize=16)\n\n    col_idx = 0\n    if has_original_image:\n        axes[col_idx].imshow(img_plot, cmap='gray' if img_plot.ndim == 2 else None)\n        axes[col_idx].set_title(\"Original Image\")\n        axes[col_idx].axis('off')\n        col_idx += 1 # Increment index -> 1\n\n    # Ground Truth Mask\n    axes[col_idx].imshow(gt_plot, cmap='gray', vmin=0, vmax=1)\n    axes[col_idx].set_title(\"Ground Truth Mask\")\n    axes[col_idx].axis('off')\n    col_idx += 1 # Increment index -> 2 (or 1 if no image)\n\n    # Prediction Probability\n    im_pred = axes[col_idx].imshow(pred_plot, cmap='viridis', vmin=0, vmax=1)\n    axes[col_idx].set_title(\"Prediction Probability\")\n    axes[col_idx].axis('off')\n    col_idx += 1 # Increment index -> 3 (or 2)\n\n    # GT Log Spectrum\n    im_gt_spec = axes[col_idx].imshow(log_spectrum_gt_np, cmap='magma')\n    axes[col_idx].set_title(\"GT Log Spectrum\")\n    axes[col_idx].axis('off')\n    fig.colorbar(im_gt_spec, ax=axes[col_idx], fraction=0.046, pad=0.04)\n    col_idx += 1 # Increment index -> 4 (or 3)\n\n    # Prediction Log Spectrum\n    im_pred_spec = axes[col_idx].imshow(log_spectrum_pred_np, cmap='magma')\n    axes[col_idx].set_title(\"Prediction Log Spectrum\")\n    axes[col_idx].axis('off')\n    fig.colorbar(im_pred_spec, ax=axes[col_idx], fraction=0.046, pad=0.04)\n    col_idx += 1 # Increment index -> 5 (or 4)\n\n    # Difference Spectrum\n    # Now col_idx should be 5 (if image shown) or 4 (if no image), which is a valid index\n    im_diff_spec = axes[col_idx].imshow(spectrum_diff_np, cmap='viridis')\n    axes[col_idx].set_title(\"Spectrum Difference (Abs Log)\")\n    axes[col_idx].axis('off')\n    fig.colorbar(im_diff_spec, ax=axes[col_idx], fraction=0.046, pad=0.04)\n    # col_idx += 1 # No increment needed after last plot\n\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n\n    if save_path:\n        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n        plt.savefig(save_path, dpi=150)\n        print(f\"Frequency visualization saved to {save_path}\")\n        #plt.close(fig)\n    plt.show()\n\n# --- Example Usage (inside your validation loop or separate script) ---\n# Assuming you have:\n# - images: Batch of original images (optional)\n# - masks: Batch of ground truth masks\n# - pred_prob_tensor: Batch of prediction probabilities (e.g., sigmoid(logits))\n# - sample_to_view: The index (e.g., 0) you want to visualize\n# - log_dir: Your logging directory\n\nvisualize_frequency_spectrum(\n    gt_mask=mask_filtered[0].detach()>0,\n    pred_prob=mask_unfiltered[0].detach()>0,\n    sample_idx=0,\n    original_image=image, # Pass if available\n    save_path=os.path.join(log_dir, f\"example_freq_analysis.png\"),\n    title_suffix=f\"Example Freq. Analysis\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Saving","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom PIL import Image\nimport os\nimport time # Optional: for timing\n\n# Assume 'modified_sam2' is your loaded model object\n# Assume '_transforms' is your initialized transforms object (handling preprocessing and postprocessing)\n# Assume 'device' is your torch device (e.g., torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n\n# Example placeholders (replace with your actual objects)\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# modified_sam2 = load_my_sam_model().to(device).eval() # Load your model and set to eval mode\n# _transforms = initialize_my_transforms() # Initialize your transforms object\n\ndef infer_and_save_masks(image_dir, output_dir, model, transforms, device):\n    \"\"\"\n    Processes all images in a directory using the provided model,\n    and saves the resulting high-resolution and low-resolution masks.\n\n    Args:\n        image_dir (str): Path to the directory containing input images.\n        output_dir (str): Path to the directory where masks will be saved.\n        model (torch.nn.Module): The loaded segmentation model (e.g., modified_sam2).\n                                 Expected to be already on the correct device and in eval mode.\n        transforms (object): An object responsible for preprocessing images\n                             (via __call__ or a specific method) and postprocessing masks\n                             (via a 'postprocess_masks' method). It should also have\n                             a 'to_tensor' method or similar if used like in the example.\n        device (torch.device): The device (CPU or CUDA) to run inference on.\n    \"\"\"\n    if not os.path.isdir(image_dir):\n        print(f\"Error: Input directory '{image_dir}' not found.\")\n        return\n\n    if not os.path.exists(output_dir):\n        print(f\"Creating output directory: {output_dir}\")\n        os.makedirs(output_dir)\n    else:\n         print(f\"Output directory '{output_dir}' already exists. Files might be overwritten.\")\n\n\n    # Ensure model is in evaluation mode\n    model.eval()\n\n    supported_extensions = ('.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff')\n    image_files = [f for f in os.listdir(image_dir) if f.lower().endswith(supported_extensions)]\n\n    print(f\"Found {len(image_files)} images in '{image_dir}'. Starting inference...\")\n    start_time = time.time()\n\n    with torch.no_grad(): # Disable gradient calculations for inference\n        for i, filename in enumerate(image_files):\n            image_path = os.path.join(image_dir, filename)\n            base_filename, _ = os.path.splitext(filename)\n            print(f\"Processing [{i+1}/{len(image_files)}]: {filename}...\")\n\n            try:\n                # 1. Load and Preprocess Image\n                image_orig = Image.open(image_path).convert(\"RGB\")\n                image_np = np.array(image_orig)\n                orig_hw = image_np.shape[:2] # Original Height, Width\n\n                # Apply preprocessing transforms (e.g., resizing, normalization)\n                # Assumes transforms object handles this when called\n                image_tensor = transforms(image_np).unsqueeze(0).to(device) # Add batch dim and move to device\n\n                # 2. Run Model Inference\n                high_res_mask_logits, low_res_mask_logits, _ = model(image_tensor) # Get raw logits\n\n                # 3. Postprocess Masks\n                # Resize masks back to original image size\n                # The postprocess_masks function likely handles removing padding and resizing\n                mask_high_res = transforms.postprocess_masks(\n                    high_res_mask_logits, orig_hw\n                )\n                mask_low_res = transforms.postprocess_masks(\n                    low_res_mask_logits, orig_hw\n                )\n\n                # 4. Threshold and Convert to Saveable Format\n                # Detach from graph, move to CPU, convert to boolean, then to uint8 (0 or 255)\n                # Squeeze to remove batch and channel dims (assuming output is [1, 1, H, W])\n                mask_high_res_np = (mask_high_res[0, 0].detach().cpu() > 0).numpy().astype(np.uint8) * 255\n                mask_low_res_np = (mask_low_res[0, 0].detach().cpu() > 0).numpy().astype(np.uint8) * 255\n\n                # 5. Save Masks as Images\n                mask_high_res_pil = Image.fromarray(255-mask_high_res_np)\n                mask_low_res_pil = Image.fromarray(255-mask_low_res_np)\n\n                output_path_high = os.path.join(output_dir, f\"{base_filename}_mask_highres.png\")\n                output_path_low = os.path.join(output_dir, f\"{base_filename}_mask_lowres.png\")\n\n                mask_high_res_pil.save(output_path_high)\n                mask_low_res_pil.save(output_path_low)\n                # print(f\"Saved masks for {filename} to {output_dir}\")\n\n            except Exception as e:\n                print(f\"Error processing {filename}: {e}\")\n                # Optionally skip to the next image or re-raise the error\n                continue\n\n    end_time = time.time()\n    total_time = end_time - start_time\n    avg_time = total_time / len(image_files) if image_files else 0\n    print(\"-\" * 30)\n    print(f\"Inference finished.\")\n    print(f\"Processed {len(image_files)} images in {total_time:.2f} seconds.\")\n    print(f\"Average time per image: {avg_time:.3f} seconds.\")\n    print(f\"Masks saved in: {output_dir}\")\n    print(\"-\" * 30)\n\n\n# 1. Define paths\nINPUT_IMAGE_DIRECTORY = \"/kaggle/input/cghd1152/drafter_17/images\" # Your input image folder\nOUTPUT_MASK_DIRECTORY = \"/kaggle/working/output_masks/\"   # Where to save masks\n\n# 3. Load your Model (Replace with your actual model loading)\ntry:\n    # Example: Assuming 'modified_sam2' is already defined and loaded elsewhere\n    # If not, load it here:\n    # from your_model_file import load_my_sam_model\n    # model_instance = load_my_sam_model(checkpoint_path=\"path/to/your/checkpoint.pth\")\n    model_instance = modified_sam2 # Use the already loaded model from your context\n    model_instance.to(device)\n    model_instance.eval() # Set to evaluation mode\n    print(\"Model loaded and set to evaluation mode.\")\nexcept NameError:\n    print(\"Error: 'modified_sam2' model not defined. Please load your model.\")\n    exit()\nexcept Exception as e:\n    print(f\"Error loading model: {e}\")\n    exit()\n\n\n# 4. Initialize your Transforms (Replace with your actual transforms)\ntry:\n    # Example: Assuming '_transforms' is already defined and initialized elsewhere\n    # If not, initialize it here:\n    # from your_transforms_file import initialize_my_transforms\n    # transforms_instance = initialize_my_transforms(target_size=1024) # Example init\n    transforms_instance = _transforms # Use the already defined transforms from your context\n    print(\"Transforms object initialized.\")\nexcept NameError:\n    print(\"Error: '_transforms' object not defined. Please initialize your transforms.\")\n    exit()\nexcept Exception as e:\n    print(f\"Error initializing transforms: {e}\")\n    exit()\n\n# 5. Run the inference and saving process\ninfer_and_save_masks(\n    image_dir=INPUT_IMAGE_DIRECTORY,\n    output_dir=OUTPUT_MASK_DIRECTORY,\n    model=model_instance,\n    transforms=transforms_instance,\n    device=device\n)\n\nprint(\"Script finished.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T11:25:13.850821Z","iopub.execute_input":"2025-05-02T11:25:13.851065Z","iopub.status.idle":"2025-05-02T11:26:51.332671Z","shell.execute_reply.started":"2025-05-02T11:25:13.851047Z","shell.execute_reply":"2025-05-02T11:26:51.331991Z"}},"outputs":[{"name":"stdout","text":"Model loaded and set to evaluation mode.\nTransforms object initialized.\nCreating output directory: /kaggle/working/output_masks/\nFound 96 images in '/kaggle/input/cghd1152/drafter_17/images'. Starting inference...\nProcessing [1/96]: C203_D2_P3.JPG...\nProcessing [2/96]: C204_D1_P4.JPG...\nProcessing [3/96]: C193_D2_P4.jpg...\nProcessing [4/96]: C196_D2_P3.JPG...\nProcessing [5/96]: C200_D1_P4.JPG...\nProcessing [6/96]: C201_D2_P2.JPG...\nProcessing [7/96]: C204_D2_P4.JPG...\nProcessing [8/96]: C202_D2_P4.JPG...\nProcessing [9/96]: C199_D2_P4.JPG...\nProcessing [10/96]: C204_D2_P3.JPG...\nProcessing [11/96]: C193_D1_P4.JPG...\nProcessing [12/96]: C196_D2_P4.JPG...\nProcessing [13/96]: C200_D2_P1.JPG...\nProcessing [14/96]: C204_D2_P1.JPG...\nProcessing [15/96]: C204_D1_P2.JPG...\nProcessing [16/96]: C198_D1_P4.jpg...\nProcessing [17/96]: C201_D1_P1.JPG...\nProcessing [18/96]: C204_D1_P1.JPG...\nProcessing [19/96]: C195_D2_P2.JPG...\nProcessing [20/96]: C195_D1_P3.JPG...\nProcessing [21/96]: C198_D2_P1.JPG...\nProcessing [22/96]: C200_D2_P3.JPG...\nProcessing [23/96]: C194_D1_P1.JPG...\nProcessing [24/96]: C202_D1_P3.JPG...\nProcessing [25/96]: C194_D1_P3.JPG...\nProcessing [26/96]: C196_D1_P2.JPG...\nProcessing [27/96]: C198_D1_P1.jpg...\nProcessing [28/96]: C200_D1_P2.JPG...\nProcessing [29/96]: C196_D1_P1.JPG...\nProcessing [30/96]: C197_D1_P1.JPG...\nProcessing [31/96]: C200_D1_P1.JPG...\nProcessing [32/96]: C194_D2_P2.JPG...\nProcessing [33/96]: C204_D2_P2.JPG...\nProcessing [34/96]: C202_D1_P4.JPG...\nProcessing [35/96]: C200_D2_P4.JPG...\nProcessing [36/96]: C199_D1_P2.jpg...\nProcessing [37/96]: C201_D2_P1.JPG...\nProcessing [38/96]: C201_D1_P2.JPG...\nProcessing [39/96]: C203_D2_P4.JPG...\nProcessing [40/96]: C198_D2_P4.JPG...\nProcessing [41/96]: C195_D1_P1.JPG...\nProcessing [42/96]: C197_D2_P3.JPG...\nProcessing [43/96]: C201_D1_P3.JPG...\nProcessing [44/96]: C199_D2_P2.JPG...\nProcessing [45/96]: C198_D1_P3.jpg...\nProcessing [46/96]: C199_D1_P1.jpg...\nProcessing [47/96]: C202_D2_P3.JPG...\nProcessing [48/96]: C198_D2_P2.JPG...\nProcessing [49/96]: C193_D1_P3.JPG...\nProcessing [50/96]: C196_D2_P2.JPG...\nProcessing [51/96]: C201_D2_P3.JPG...\nProcessing [52/96]: C195_D2_P1.JPG...\nProcessing [53/96]: C204_D1_P3.JPG...\nProcessing [54/96]: C197_D2_P2.JPG...\nProcessing [55/96]: C203_D2_P1.JPG...\nProcessing [56/96]: C194_D1_P4.JPG...\nProcessing [57/96]: C197_D2_P1.JPG...\nProcessing [58/96]: C197_D1_P2.JPG...\nProcessing [59/96]: C196_D2_P1.JPG...\nProcessing [60/96]: C197_D1_P3.JPG...\nProcessing [61/96]: C199_D1_P4.jpg...\nProcessing [62/96]: C196_D1_P4.JPG...\nProcessing [63/96]: C195_D1_P2.jpg...\nProcessing [64/96]: C195_D2_P3.JPG...\nProcessing [65/96]: C193_D1_P2.JPG...\nProcessing [66/96]: C199_D1_P3.jpg...\nProcessing [67/96]: C194_D2_P3.JPG...\nProcessing [68/96]: C197_D1_P4.JPG...\nProcessing [69/96]: C199_D2_P3.JPG...\nProcessing [70/96]: C199_D2_P1.JPG...\nProcessing [71/96]: C193_D1_P1.JPG...\nProcessing [72/96]: C203_D2_P2.JPG...\nProcessing [73/96]: C197_D2_P4.JPG...\nProcessing [74/96]: C195_D1_P4.JPG...\nProcessing [75/96]: C203_D1_P4.JPG...\nProcessing [76/96]: C202_D1_P1.JPG...\nProcessing [77/96]: C203_D1_P3.JPG...\nProcessing [78/96]: C193_D2_P3.jpg...\nProcessing [79/96]: C200_D1_P3.JPG...\nProcessing [80/96]: C198_D1_P2.jpg...\nProcessing [81/96]: C201_D2_P4.JPG...\nProcessing [82/96]: C193_D2_P1.png...\nProcessing [83/96]: C193_D2_P2.jpg...\nProcessing [84/96]: C194_D2_P4.JPG...\nProcessing [85/96]: C201_D1_P4.JPG...\nProcessing [86/96]: C202_D1_P2.JPG...\nProcessing [87/96]: C195_D2_P4.JPG...\nProcessing [88/96]: C194_D2_P1.JPG...\nProcessing [89/96]: C202_D2_P2.JPG...\nProcessing [90/96]: C194_D1_P2.JPG...\nProcessing [91/96]: C196_D1_P3.JPG...\nProcessing [92/96]: C198_D2_P3.JPG...\nProcessing [93/96]: C202_D2_P1.JPG...\nProcessing [94/96]: C203_D1_P1.JPG...\nProcessing [95/96]: C203_D1_P2.JPG...\nProcessing [96/96]: C200_D2_P2.JPG...\n------------------------------\nInference finished.\nProcessed 96 images in 97.43 seconds.\nAverage time per image: 1.015 seconds.\nMasks saved in: /kaggle/working/output_masks/\n------------------------------\nScript finished.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"!zip -r /kaggle/working/output_masks.zip /kaggle/working/output_masks/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T11:26:51.333503Z","iopub.execute_input":"2025-05-02T11:26:51.333774Z","iopub.status.idle":"2025-05-02T11:26:51.820609Z","shell.execute_reply.started":"2025-05-02T11:26:51.333746Z","shell.execute_reply":"2025-05-02T11:26:51.819900Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/output_masks/ (stored 0%)\n  adding: kaggle/working/output_masks/C198_D2_P2_mask_lowres.png (deflated 34%)\n  adding: kaggle/working/output_masks/C204_D2_P4_mask_lowres.png (deflated 9%)\n  adding: kaggle/working/output_masks/C196_D2_P2_mask_lowres.png (deflated 28%)\n  adding: kaggle/working/output_masks/C197_D2_P4_mask_highres.png (deflated 34%)\n  adding: kaggle/working/output_masks/C198_D2_P4_mask_lowres.png (deflated 33%)\n  adding: kaggle/working/output_masks/C193_D2_P3_mask_lowres.png (deflated 2%)\n  adding: kaggle/working/output_masks/C193_D1_P1_mask_highres.png (deflated 11%)\n  adding: kaggle/working/output_masks/C193_D1_P2_mask_lowres.png (deflated 14%)\n  adding: kaggle/working/output_masks/C196_D2_P3_mask_highres.png (deflated 33%)\n  adding: kaggle/working/output_masks/C193_D2_P1_mask_highres.png (deflated 2%)\n  adding: kaggle/working/output_masks/C195_D2_P3_mask_highres.png (deflated 22%)\n  adding: kaggle/working/output_masks/C199_D2_P4_mask_highres.png (deflated 36%)\n  adding: kaggle/working/output_masks/C204_D1_P1_mask_lowres.png (deflated 9%)\n  adding: kaggle/working/output_masks/C196_D1_P1_mask_lowres.png (deflated 19%)\n  adding: kaggle/working/output_masks/C196_D2_P4_mask_highres.png (deflated 32%)\n  adding: kaggle/working/output_masks/C197_D2_P4_mask_lowres.png (deflated 31%)\n  adding: kaggle/working/output_masks/C201_D1_P2_mask_highres.png (deflated 21%)\n  adding: kaggle/working/output_masks/C202_D2_P3_mask_lowres.png (deflated 10%)\n  adding: kaggle/working/output_masks/C196_D1_P4_mask_highres.png (deflated 34%)\n  adding: kaggle/working/output_masks/C200_D1_P2_mask_lowres.png (deflated 20%)\n  adding: kaggle/working/output_masks/C199_D2_P2_mask_highres.png (deflated 39%)\n  adding: kaggle/working/output_masks/C198_D1_P2_mask_highres.png (deflated 11%)\n  adding: kaggle/working/output_masks/C202_D1_P1_mask_highres.png (deflated 25%)\n  adding: kaggle/working/output_masks/C201_D1_P3_mask_highres.png (deflated 23%)\n  adding: kaggle/working/output_masks/C203_D1_P2_mask_highres.png (deflated 22%)\n  adding: kaggle/working/output_masks/C196_D2_P3_mask_lowres.png (deflated 31%)\n  adding: kaggle/working/output_masks/C193_D1_P4_mask_lowres.png (deflated 10%)\n  adding: kaggle/working/output_masks/C195_D1_P3_mask_highres.png (deflated 20%)\n  adding: kaggle/working/output_masks/C200_D2_P3_mask_highres.png (deflated 17%)\n  adding: kaggle/working/output_masks/C201_D2_P4_mask_lowres.png (deflated 16%)\n  adding: kaggle/working/output_masks/C196_D1_P2_mask_highres.png (deflated 27%)\n  adding: kaggle/working/output_masks/C201_D2_P4_mask_highres.png (deflated 17%)\n  adding: kaggle/working/output_masks/C193_D2_P1_mask_lowres.png (deflated 2%)\n  adding: kaggle/working/output_masks/C201_D2_P3_mask_highres.png (deflated 23%)\n  adding: kaggle/working/output_masks/C195_D2_P4_mask_lowres.png (deflated 14%)\n  adding: kaggle/working/output_masks/C194_D2_P3_mask_highres.png (deflated 20%)\n  adding: kaggle/working/output_masks/C194_D2_P1_mask_highres.png (deflated 20%)\n  adding: kaggle/working/output_masks/C201_D1_P4_mask_lowres.png (deflated 19%)\n  adding: kaggle/working/output_masks/C204_D1_P1_mask_highres.png (deflated 9%)\n  adding: kaggle/working/output_masks/C203_D1_P3_mask_lowres.png (deflated 16%)\n  adding: kaggle/working/output_masks/C203_D2_P4_mask_highres.png (deflated 18%)\n  adding: kaggle/working/output_masks/C193_D1_P4_mask_highres.png (deflated 12%)\n  adding: kaggle/working/output_masks/C198_D1_P4_mask_highres.png (deflated 8%)\n  adding: kaggle/working/output_masks/C198_D1_P2_mask_lowres.png (deflated 11%)\n  adding: kaggle/working/output_masks/C194_D1_P4_mask_highres.png (deflated 18%)\n  adding: kaggle/working/output_masks/C196_D1_P4_mask_lowres.png (deflated 32%)\n  adding: kaggle/working/output_masks/C193_D2_P4_mask_highres.png (deflated 1%)\n  adding: kaggle/working/output_masks/C203_D1_P4_mask_lowres.png (deflated 18%)\n  adding: kaggle/working/output_masks/C201_D2_P1_mask_highres.png (deflated 20%)\n  adding: kaggle/working/output_masks/C203_D2_P3_mask_lowres.png (deflated 13%)\n  adding: kaggle/working/output_masks/C202_D1_P3_mask_highres.png (deflated 26%)\n  adding: kaggle/working/output_masks/C194_D2_P1_mask_lowres.png (deflated 15%)\n  adding: kaggle/working/output_masks/C201_D2_P1_mask_lowres.png (deflated 17%)\n  adding: kaggle/working/output_masks/C197_D1_P1_mask_lowres.png (deflated 31%)\n  adding: kaggle/working/output_masks/C202_D1_P3_mask_lowres.png (deflated 22%)\n  adding: kaggle/working/output_masks/C195_D2_P1_mask_highres.png (deflated 25%)\n  adding: kaggle/working/output_masks/C193_D2_P4_mask_lowres.png (deflated 1%)\n  adding: kaggle/working/output_masks/C200_D2_P3_mask_lowres.png (deflated 16%)\n  adding: kaggle/working/output_masks/C200_D2_P2_mask_lowres.png (deflated 13%)\n  adding: kaggle/working/output_masks/C198_D2_P3_mask_lowres.png (deflated 27%)\n  adding: kaggle/working/output_masks/C197_D2_P1_mask_highres.png (deflated 31%)\n  adding: kaggle/working/output_masks/C202_D1_P4_mask_lowres.png (deflated 26%)\n  adding: kaggle/working/output_masks/C193_D2_P2_mask_lowres.png (deflated 2%)\n  adding: kaggle/working/output_masks/C195_D1_P1_mask_highres.png (deflated 20%)\n  adding: kaggle/working/output_masks/C204_D2_P3_mask_highres.png (deflated 12%)\n  adding: kaggle/working/output_masks/C195_D1_P2_mask_highres.png (deflated 4%)\n  adding: kaggle/working/output_masks/C204_D1_P3_mask_highres.png (deflated 30%)\n  adding: kaggle/working/output_masks/C193_D2_P2_mask_highres.png (deflated 2%)\n  adding: kaggle/working/output_masks/C200_D1_P2_mask_highres.png (deflated 23%)\n  adding: kaggle/working/output_masks/C198_D1_P1_mask_lowres.png (deflated 13%)\n  adding: kaggle/working/output_masks/C203_D1_P4_mask_highres.png (deflated 22%)\n  adding: kaggle/working/output_masks/C194_D2_P4_mask_highres.png (deflated 11%)\n  adding: kaggle/working/output_masks/C198_D2_P2_mask_highres.png (deflated 40%)\n  adding: kaggle/working/output_masks/C202_D1_P4_mask_highres.png (deflated 29%)\n  adding: kaggle/working/output_masks/C199_D1_P4_mask_highres.png (deflated 12%)\n  adding: kaggle/working/output_masks/C203_D2_P1_mask_highres.png (deflated 16%)\n  adding: kaggle/working/output_masks/C197_D1_P4_mask_lowres.png (deflated 11%)\n  adding: kaggle/working/output_masks/C199_D1_P3_mask_highres.png (deflated 3%)\n  adding: kaggle/working/output_masks/C194_D1_P1_mask_highres.png (deflated 16%)\n  adding: kaggle/working/output_masks/C194_D2_P2_mask_lowres.png (deflated 17%)\n  adding: kaggle/working/output_masks/C195_D1_P1_mask_lowres.png (deflated 16%)\n  adding: kaggle/working/output_masks/C203_D2_P4_mask_lowres.png (deflated 14%)\n  adding: kaggle/working/output_masks/C202_D2_P1_mask_highres.png (deflated 13%)\n  adding: kaggle/working/output_masks/C195_D2_P3_mask_lowres.png (deflated 19%)\n  adding: kaggle/working/output_masks/C204_D2_P2_mask_highres.png (deflated 14%)\n  adding: kaggle/working/output_masks/C200_D1_P1_mask_lowres.png (deflated 19%)\n  adding: kaggle/working/output_masks/C203_D2_P2_mask_highres.png (deflated 14%)\n  adding: kaggle/working/output_masks/C195_D1_P4_mask_highres.png (deflated 23%)\n  adding: kaggle/working/output_masks/C202_D2_P2_mask_highres.png (deflated 11%)\n  adding: kaggle/working/output_masks/C197_D1_P4_mask_highres.png (deflated 12%)\n  adding: kaggle/working/output_masks/C197_D2_P3_mask_lowres.png (deflated 30%)\n  adding: kaggle/working/output_masks/C194_D2_P4_mask_lowres.png (deflated 10%)\n  adding: kaggle/working/output_masks/C199_D2_P1_mask_lowres.png (deflated 29%)\n  adding: kaggle/working/output_masks/C198_D2_P4_mask_highres.png (deflated 37%)\n  adding: kaggle/working/output_masks/C199_D1_P1_mask_lowres.png (deflated 6%)\n  adding: kaggle/working/output_masks/C200_D2_P1_mask_highres.png (deflated 18%)\n  adding: kaggle/working/output_masks/C195_D2_P1_mask_lowres.png (deflated 21%)\n  adding: kaggle/working/output_masks/C199_D1_P4_mask_lowres.png (deflated 11%)\n  adding: kaggle/working/output_masks/C203_D2_P2_mask_lowres.png (deflated 11%)\n  adding: kaggle/working/output_masks/C197_D1_P2_mask_lowres.png (deflated 28%)\n  adding: kaggle/working/output_masks/C196_D2_P2_mask_highres.png (deflated 33%)\n  adding: kaggle/working/output_masks/C201_D1_P2_mask_lowres.png (deflated 19%)\n  adding: kaggle/working/output_masks/C204_D2_P2_mask_lowres.png (deflated 11%)\n  adding: kaggle/working/output_masks/C201_D1_P3_mask_lowres.png (deflated 20%)\n  adding: kaggle/working/output_masks/C196_D2_P1_mask_lowres.png (deflated 31%)\n  adding: kaggle/working/output_masks/C194_D1_P4_mask_lowres.png (deflated 15%)\n  adding: kaggle/working/output_masks/C199_D2_P2_mask_lowres.png (deflated 33%)\n  adding: kaggle/working/output_masks/C204_D1_P3_mask_lowres.png (deflated 26%)\n  adding: kaggle/working/output_masks/C196_D1_P3_mask_lowres.png (deflated 30%)\n  adding: kaggle/working/output_masks/C194_D2_P3_mask_lowres.png (deflated 15%)\n  adding: kaggle/working/output_masks/C195_D1_P3_mask_lowres.png (deflated 17%)\n  adding: kaggle/working/output_masks/C203_D1_P3_mask_highres.png (deflated 19%)\n  adding: kaggle/working/output_masks/C194_D1_P2_mask_highres.png (deflated 18%)\n  adding: kaggle/working/output_masks/C193_D1_P3_mask_highres.png (deflated 9%)\n  adding: kaggle/working/output_masks/C204_D1_P4_mask_highres.png (deflated 23%)\n  adding: kaggle/working/output_masks/C196_D2_P4_mask_lowres.png (deflated 28%)\n  adding: kaggle/working/output_masks/C194_D2_P2_mask_highres.png (deflated 22%)\n  adding: kaggle/working/output_masks/C197_D2_P1_mask_lowres.png (deflated 26%)\n  adding: kaggle/working/output_masks/C196_D1_P3_mask_highres.png (deflated 33%)\n  adding: kaggle/working/output_masks/C200_D2_P1_mask_lowres.png (deflated 16%)\n  adding: kaggle/working/output_masks/C194_D1_P1_mask_lowres.png (deflated 13%)\n  adding: kaggle/working/output_masks/C198_D1_P1_mask_highres.png (deflated 15%)\n  adding: kaggle/working/output_masks/C204_D1_P4_mask_lowres.png (deflated 19%)\n  adding: kaggle/working/output_masks/C204_D2_P3_mask_lowres.png (deflated 9%)\n  adding: kaggle/working/output_masks/C199_D2_P4_mask_lowres.png (deflated 32%)\n  adding: kaggle/working/output_masks/C198_D1_P3_mask_lowres.png (deflated 6%)\n  adding: kaggle/working/output_masks/C196_D1_P1_mask_highres.png (deflated 21%)\n  adding: kaggle/working/output_masks/C199_D1_P3_mask_lowres.png (deflated 2%)\n  adding: kaggle/working/output_masks/C193_D1_P1_mask_lowres.png (deflated 10%)\n  adding: kaggle/working/output_masks/C194_D1_P3_mask_lowres.png (deflated 15%)\n  adding: kaggle/working/output_masks/C200_D2_P2_mask_highres.png (deflated 15%)\n  adding: kaggle/working/output_masks/C197_D2_P3_mask_highres.png (deflated 35%)\n  adding: kaggle/working/output_masks/C193_D1_P2_mask_highres.png (deflated 15%)\n  adding: kaggle/working/output_masks/C200_D1_P3_mask_lowres.png (deflated 22%)\n  adding: kaggle/working/output_masks/C204_D1_P2_mask_lowres.png (deflated 15%)\n  adding: kaggle/working/output_masks/C196_D1_P2_mask_lowres.png (deflated 23%)\n  adding: kaggle/working/output_masks/C195_D1_P4_mask_lowres.png (deflated 20%)\n  adding: kaggle/working/output_masks/C195_D2_P2_mask_lowres.png (deflated 18%)\n  adding: kaggle/working/output_masks/C200_D2_P4_mask_lowres.png (deflated 19%)\n  adding: kaggle/working/output_masks/C203_D2_P1_mask_lowres.png (deflated 12%)\n  adding: kaggle/working/output_masks/C204_D2_P1_mask_highres.png (deflated 13%)\n  adding: kaggle/working/output_masks/C198_D2_P3_mask_highres.png (deflated 29%)\n  adding: kaggle/working/output_masks/C197_D1_P3_mask_highres.png (deflated 9%)\n  adding: kaggle/working/output_masks/C203_D1_P1_mask_highres.png (deflated 24%)\n  adding: kaggle/working/output_masks/C201_D2_P2_mask_lowres.png (deflated 18%)\n  adding: kaggle/working/output_masks/C199_D2_P1_mask_highres.png (deflated 33%)\n  adding: kaggle/working/output_masks/C202_D2_P4_mask_highres.png (deflated 10%)\n  adding: kaggle/working/output_masks/C194_D1_P2_mask_lowres.png (deflated 15%)\n  adding: kaggle/working/output_masks/C199_D1_P1_mask_highres.png (deflated 6%)\n  adding: kaggle/working/output_masks/C193_D1_P3_mask_lowres.png (deflated 8%)\n  adding: kaggle/working/output_masks/C197_D1_P1_mask_highres.png (deflated 33%)\n  adding: kaggle/working/output_masks/C201_D2_P2_mask_highres.png (deflated 21%)\n  adding: kaggle/working/output_masks/C199_D1_P2_mask_highres.png (deflated 4%)\n  adding: kaggle/working/output_masks/C200_D1_P4_mask_highres.png (deflated 26%)\n  adding: kaggle/working/output_masks/C198_D2_P1_mask_lowres.png (deflated 29%)\n  adding: kaggle/working/output_masks/C198_D1_P3_mask_highres.png (deflated 8%)\n  adding: kaggle/working/output_masks/C198_D2_P1_mask_highres.png (deflated 34%)\n  adding: kaggle/working/output_masks/C199_D2_P3_mask_lowres.png (deflated 29%)\n  adding: kaggle/working/output_masks/C197_D1_P3_mask_lowres.png (deflated 8%)\n  adding: kaggle/working/output_masks/C195_D2_P4_mask_highres.png (deflated 14%)\n  adding: kaggle/working/output_masks/C199_D1_P2_mask_lowres.png (deflated 3%)\n  adding: kaggle/working/output_masks/C202_D2_P1_mask_lowres.png (deflated 9%)\n  adding: kaggle/working/output_masks/C200_D1_P1_mask_highres.png (deflated 23%)\n  adding: kaggle/working/output_masks/C201_D1_P1_mask_highres.png (deflated 24%)\n  adding: kaggle/working/output_masks/C202_D2_P2_mask_lowres.png (deflated 9%)\n  adding: kaggle/working/output_masks/C201_D2_P3_mask_lowres.png (deflated 19%)\n  adding: kaggle/working/output_masks/C197_D2_P2_mask_lowres.png (deflated 31%)\n  adding: kaggle/working/output_masks/C197_D1_P2_mask_highres.png (deflated 30%)\n  adding: kaggle/working/output_masks/C202_D1_P2_mask_lowres.png (deflated 22%)\n  adding: kaggle/working/output_masks/C204_D2_P4_mask_highres.png (deflated 11%)\n  adding: kaggle/working/output_masks/C202_D1_P1_mask_lowres.png (deflated 21%)\n  adding: kaggle/working/output_masks/C201_D1_P4_mask_highres.png (deflated 22%)\n  adding: kaggle/working/output_masks/C202_D2_P4_mask_lowres.png (deflated 8%)\n  adding: kaggle/working/output_masks/C200_D2_P4_mask_highres.png (deflated 23%)\n  adding: kaggle/working/output_masks/C202_D2_P3_mask_highres.png (deflated 14%)\n  adding: kaggle/working/output_masks/C204_D1_P2_mask_highres.png (deflated 16%)\n  adding: kaggle/working/output_masks/C195_D2_P2_mask_highres.png (deflated 23%)\n  adding: kaggle/working/output_masks/C196_D2_P1_mask_highres.png (deflated 37%)\n  adding: kaggle/working/output_masks/C199_D2_P3_mask_highres.png (deflated 32%)\n  adding: kaggle/working/output_masks/C201_D1_P1_mask_lowres.png (deflated 20%)\n  adding: kaggle/working/output_masks/C203_D2_P3_mask_highres.png (deflated 17%)\n  adding: kaggle/working/output_masks/C203_D1_P2_mask_lowres.png (deflated 20%)\n  adding: kaggle/working/output_masks/C197_D2_P2_mask_highres.png (deflated 35%)\n  adding: kaggle/working/output_masks/C195_D1_P2_mask_lowres.png (deflated 3%)\n  adding: kaggle/working/output_masks/C202_D1_P2_mask_highres.png (deflated 26%)\n  adding: kaggle/working/output_masks/C198_D1_P4_mask_lowres.png (deflated 7%)\n  adding: kaggle/working/output_masks/C200_D1_P3_mask_highres.png (deflated 26%)\n  adding: kaggle/working/output_masks/C193_D2_P3_mask_highres.png (deflated 2%)\n  adding: kaggle/working/output_masks/C203_D1_P1_mask_lowres.png (deflated 20%)\n  adding: kaggle/working/output_masks/C194_D1_P3_mask_highres.png (deflated 18%)\n  adding: kaggle/working/output_masks/C204_D2_P1_mask_lowres.png (deflated 10%)\n  adding: kaggle/working/output_masks/C200_D1_P4_mask_lowres.png (deflated 22%)\n","output_type":"stream"}],"execution_count":22}]}